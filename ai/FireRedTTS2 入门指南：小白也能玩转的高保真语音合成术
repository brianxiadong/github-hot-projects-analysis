# FireRedTTS2 å…¥é—¨æŒ‡å—ï¼šå°ç™½ä¹Ÿèƒ½ç©è½¬çš„é«˜ä¿çœŸè¯­éŸ³åˆæˆæœ¯

## æ¦‚è¿°

[FireRedTTS2](https://github.com/FireRedTeam/FireRedTTS2) æ˜¯ä¸€ä¸ªé¢å‘é•¿å¯¹è¯è¯­éŸ³ç”Ÿæˆçš„æµå¼æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç³»ç»Ÿï¼Œä¸“ä¸ºæ’­å®¢å’ŒèŠå¤©æœºå™¨äººåœºæ™¯è®¾è®¡ã€‚å®ƒåŸºäº PyTorch å®ç°ï¼Œæ”¯æŒå¤šè¯´è¯äººã€å¤šè¯­è¨€çš„è‡ªç„¶å¯¹è¯ç”Ÿæˆã€‚

### ä¸»è¦ç‰¹æ€§

- **é•¿å¯¹è¯ç”Ÿæˆ**ï¼šæ”¯æŒæœ€å¤š4äººã€3åˆ†é’Ÿä»¥ä¸Šçš„å¤šè¯´è¯äººå¯¹è¯ç”Ÿæˆ
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ã€æ³•æ–‡ã€å¾·æ–‡ã€ä¿„æ–‡ç­‰å¤šç§è¯­è¨€
- **é›¶æ ·æœ¬è¯­éŸ³å…‹éš†**ï¼šæ”¯æŒè·¨è¯­è¨€å’Œä»£ç æ··åˆåœºæ™¯ä¸‹çš„è¯­éŸ³å…‹éš†
- **è¶…ä½å»¶è¿Ÿ**ï¼šåŸºäº12.5Hzæµå¼è¯­éŸ³tokenizerï¼Œé¦–åŒ…å»¶è¿Ÿä½è‡³140ms
- **é«˜ç¨³å®šæ€§**ï¼šåœ¨ç‹¬ç™½å’Œå¯¹è¯æµ‹è¯•ä¸­å®ç°é«˜ç›¸ä¼¼åº¦å’Œä½WER/CER
- **éšæœºéŸ³è‰²ç”Ÿæˆ**ï¼šé€‚ç”¨äºASR/è¯­éŸ³äº¤äº’æ•°æ®ç”Ÿæˆ

### æŠ€æœ¯äº®ç‚¹

- **åŒTransformeræ¶æ„**ï¼šå¤„ç†æ–‡æœ¬ä¸è¯­éŸ³äº¤é”™åºåˆ—ï¼Œå®ç°ç«¯åˆ°ç«¯æµå¼è¯­éŸ³ç”Ÿæˆ
- **æµå¼å¤„ç†**ï¼šæ”¯æŒé€å¥ç”Ÿæˆï¼Œé™ä½é¦–åŒ…å»¶è¿Ÿ
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šLLMä¸Codecè§£è€¦ï¼Œä¾¿äºæ‰©å±•
- **å¯¹è¯çŠ¶æ€ç®¡ç†**ï¼šé€šè¿‡`[S1]`ã€`[S2]`æ ‡è®°å®ç°è¯´è¯äººæ§åˆ¶

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**ï¼šæ”¯æŒ CUDA çš„ Linux/macOS/Windows
- **Pythonç‰ˆæœ¬**ï¼šPython 3.11
- **ç¡¬ä»¶è¦æ±‚**ï¼š
  - NVIDIA GPUï¼ˆæ¨è L20 æˆ–åŒç­‰æ€§èƒ½æ˜¾å¡ï¼‰
  - æ˜¾å­˜ï¼šæ¨ç†æ—¶å»ºè®® â‰¥24GB
  - ç£ç›˜ç©ºé—´ï¼šâ‰¥10GBï¼ˆç”¨äºå­˜å‚¨æ¨¡å‹ï¼‰

### å¿«é€Ÿå®‰è£…

1. **å…‹éš†é¡¹ç›®**
   ```bash
   git clone https://github.com/FireRedTeam/FireRedTTS2.git
   cd FireRedTTS2
   ```

2. **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ**
   ```bash
   conda create --name fireredtts2 python==3.11
   conda activate fireredtts2
   ```

3. **å®‰è£…PyTorch**
   ```bash
   pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu126
   ```

4. **å®‰è£…ä¾èµ–**
   ```bash
   pip install -e .
   pip install -r requirements.txt
   ```

5. **ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹**
   ```bash
   git lfs install
   git clone https://huggingface.co/FireRedTeam/FireRedTTS2 pretrained_models/FireRedTTS2
   ```
	å›½å†…å¯ä»¥ä½¿ç”¨modelscopeå®‰è£…:
	```bash
	pip install modelscope
	modelscope download --model FireRedTeam/FireRedTTS2  --local_dir pretrained_models/FireRedTTS2
	```
### éªŒè¯å®‰è£…

è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯å®‰è£…æ˜¯å¦æˆåŠŸï¼š

```bash
python gradio_demo.py --pretrained-dir "./pretrained_models/FireRedTTS2"
```

å¦‚æœå®‰è£…æˆåŠŸï¼Œå°†çœ‹åˆ° Web UI å¯åŠ¨çš„æç¤ºä¿¡æ¯ã€‚

## å®‰è£…é€‰é¡¹

### å®Œæ•´å®‰è£…ï¼ˆæ¨èï¼‰

é€‚ç”¨äºå¤§å¤šæ•°ç”¨æˆ·ï¼ŒåŒ…å«æ‰€æœ‰åŠŸèƒ½å’Œä¾èµ–ï¼š

```bash
# å®Œæ•´æ­¥éª¤å¦‚ä¸Šé¢å¿«é€Ÿå®‰è£…æ‰€ç¤º
```

### æœ€å°åŒ–å®‰è£…

å¦‚æœåªéœ€è¦æ ¸å¿ƒåŠŸèƒ½ï¼Œå¯ä»¥é€‰æ‹©æœ€å°åŒ–å®‰è£…ï¼š

```bash
# ä»…å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio transformers
pip install -e .
```

### å¼€å‘è€…å®‰è£…

é€‚ç”¨äºéœ€è¦ä¿®æ”¹æºç çš„å¼€å‘è€…ï¼š

```bash
# å¼€å‘æ¨¡å¼å®‰è£…
pip install -e . --dev
pip install pytest black flake8  # å¯é€‰ï¼šå¼€å‘å·¥å…·
```

### æ›¿ä»£å®‰è£…æ–¹å¼

**ä½¿ç”¨ Hugging Face CLI ä¸‹è½½æ¨¡å‹ï¼ˆæ¨èï¼‰**ï¼š

```bash
# å¦‚æœ git lfs ä¸‹è½½è¿‡æ…¢ï¼Œå¯ä½¿ç”¨ä»¥ä¸‹æ–¹å¼
huggingface-cli download FireRedTeam/FireRedTTS2 --local-dir pretrained_models/FireRedTTS2
```

**Docker å®‰è£…**ï¼ˆé€‚ç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰ï¼š

```bash
# æ„å»º Docker é•œåƒï¼ˆéœ€è¦å…ˆåˆ›å»º Dockerfileï¼‰
docker build -t fireredtts2 .
docker run --gpus all -p 7860:7860 fireredtts2
```

## ä½¿ç”¨Gradioæ¼”ç¤º

### å¯åŠ¨Webç•Œé¢

```bash
python gradio_demo.py --pretrained-dir "./pretrained_models/FireRedTTS2"
```

å¯åŠ¨åï¼Œæµè§ˆå™¨ä¼šè‡ªåŠ¨æ‰“å¼€ `http://127.0.0.1:7860`ï¼Œæ‚¨å°†çœ‹åˆ° FireRedTTS2 çš„ Web ç•Œé¢ã€‚

### ç•Œé¢åŠŸèƒ½è¯´æ˜

#### 1. è¯­è¨€é€‰æ‹©
- æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ç•Œé¢åˆ‡æ¢
- ç•Œé¢å³ä¸Šè§’å¯é€‰æ‹©æ˜¾ç¤ºè¯­è¨€

#### 2. éŸ³è‰²æ¨¡å¼
- **éŸ³è‰²å…‹éš†**ï¼šä½¿ç”¨ä¸Šä¼ çš„å‚è€ƒéŸ³é¢‘å…‹éš†æŒ‡å®šè¯´è¯äººéŸ³è‰²
- **éšæœºéŸ³è‰²**ï¼šç³»ç»Ÿè‡ªåŠ¨ç”ŸæˆéšæœºéŸ³è‰²

#### 3. éŸ³è‰²å…‹éš†æ¨¡å¼æ“ä½œ

**è¯´è¯äºº1è®¾ç½®**ï¼š
- ä¸Šä¼ å‚è€ƒéŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒ wavã€mp3ã€flac æ ¼å¼ï¼‰
- è¾“å…¥å‚è€ƒæ–‡æœ¬ï¼Œæ ¼å¼ï¼š`[S1]å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬å†…å®¹`

**è¯´è¯äºº2è®¾ç½®**ï¼š
- ä¸Šä¼ å‚è€ƒéŸ³é¢‘æ–‡ä»¶
- è¾“å…¥å‚è€ƒæ–‡æœ¬ï¼Œæ ¼å¼ï¼š`[S2]å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬å†…å®¹`

#### 4. å¯¹è¯æ–‡æœ¬è¾“å…¥
åœ¨æ–‡æœ¬æ¡†ä¸­è¾“å…¥å¯¹è¯å†…å®¹ï¼Œæ ¼å¼ä¸ºï¼š
```
[S1]ç¬¬ä¸€ä¸ªè¯´è¯äººçš„è¯[S2]ç¬¬äºŒä¸ªè¯´è¯äººçš„è¯[S1]ç»§ç»­å¯¹è¯...
```

#### 5. ç”ŸæˆéŸ³é¢‘
- ç‚¹å‡»"åˆæˆ"æŒ‰é’®å¼€å§‹ç”Ÿæˆ
- ç”Ÿæˆè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºè¿›åº¦æ¡
- å®Œæˆåå¯æ’­æ”¾æˆ–ä¸‹è½½ç”Ÿæˆçš„éŸ³é¢‘

### ä½¿ç”¨ç¤ºä¾‹

#### éŸ³è‰²å…‹éš†ç¤ºä¾‹
1. é€‰æ‹©"éŸ³è‰²å…‹éš†"æ¨¡å¼
2. ä¸ºè¯´è¯äºº1ä¸Šä¼ éŸ³é¢‘å’Œå¯¹åº”æ–‡æœ¬
3. ä¸ºè¯´è¯äºº2ä¸Šä¼ éŸ³é¢‘å’Œå¯¹åº”æ–‡æœ¬
4. è¾“å…¥å¯¹è¯æ–‡æœ¬ï¼š
   ```
   [S1]ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ[S2]å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšã€‚[S1]é‚£æˆ‘ä»¬å‡ºå»èµ°èµ°å§ã€‚
   ```
5. ç‚¹å‡»"åˆæˆ"ç”Ÿæˆå¯¹è¯éŸ³é¢‘

#### éšæœºéŸ³è‰²ç¤ºä¾‹
1. é€‰æ‹©"éšæœºéŸ³è‰²"æ¨¡å¼
2. ç›´æ¥è¾“å…¥å¯¹è¯æ–‡æœ¬ï¼š
   ```
   [S1]Welcome to FireRedTTS2![S2]This is an amazing text-to-speech system.
   ```
3. ç‚¹å‡»"åˆæˆ"ç”Ÿæˆå¯¹è¯éŸ³é¢‘

## æ·±å…¥æ¢ç´¢

### ä»£ç ä½¿ç”¨ç¤ºä¾‹

#### å¯¹è¯ç”Ÿæˆ

```python
import torch
import torchaudio
from fireredtts2.fireredtts2 import FireRedTTS2

# åˆå§‹åŒ–æ¨¡å‹
device = "cuda"
fireredtts2 = FireRedTTS2(
    pretrained_dir="./pretrained_models/FireRedTTS2",
    gen_type="dialogue",
    device=device,
)

# å‡†å¤‡å¯¹è¯æ–‡æœ¬
text_list = [
    "[S1]ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨FireRedTTS2ï¼",
    "[S2]è¿™ä¸ªç³»ç»ŸçœŸçš„å¾ˆæ£’ï¼Œæ”¯æŒå¤šè¯­è¨€å¯¹è¯ç”Ÿæˆã€‚",
    "[S1]æ˜¯çš„ï¼Œè€Œä¸”å»¶è¿Ÿå¾ˆä½ï¼Œéå¸¸é€‚åˆå®æ—¶åº”ç”¨ã€‚"
]

# éŸ³è‰²å…‹éš†æ¨¡å¼
prompt_wav_list = [
    "path/to/speaker1_reference.wav",
    "path/to/speaker2_reference.wav"
]
prompt_text_list = [
    "[S1]è¿™æ˜¯è¯´è¯äºº1çš„å‚è€ƒæ–‡æœ¬",
    "[S2]è¿™æ˜¯è¯´è¯äºº2çš„å‚è€ƒæ–‡æœ¬"
]

# ç”Ÿæˆå¯¹è¯éŸ³é¢‘
audio = fireredtts2.generate_dialogue(
    text_list=text_list,
    prompt_wav_list=prompt_wav_list,
    prompt_text_list=prompt_text_list,
    temperature=0.9,
    topk=30,
)

# ä¿å­˜éŸ³é¢‘
torchaudio.save("dialogue_output.wav", audio, 24000)
```

#### ç‹¬ç™½ç”Ÿæˆ

```python
from fireredtts2.fireredtts2 import FireRedTTS2

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆç‹¬ç™½æ¨¡å¼ï¼‰
fireredtts2 = FireRedTTS2(
    pretrained_dir="./pretrained_models/FireRedTTS2",
    gen_type="monologue",
    device="cuda",
)

# éšæœºéŸ³è‰²ç”Ÿæˆ
text = "Hello, this is a demonstration of FireRedTTS2 monologue generation."
audio = fireredtts2.generate_monologue(text=text)
torchaudio.save("monologue_random.wav", audio, 24000)

# éŸ³è‰²å…‹éš†ç”Ÿæˆ
audio = fireredtts2.generate_monologue(
    text=text,
    prompt_wav="path/to/reference.wav",
    prompt_text="å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬",
)
torchaudio.save("monologue_clone.wav", audio, 24000)
```

### é«˜çº§å‚æ•°è°ƒèŠ‚ä¸æ€§èƒ½ä¼˜åŒ–

#### ç²¾ç»†å‚æ•°æ§åˆ¶ç­–ç•¥

**1. ç”Ÿæˆè´¨é‡æ§åˆ¶**

```python
# ä¸åŒåœºæ™¯çš„å‚æ•°ç»„åˆ
def get_optimal_params(scenario):
    params = {
        "podcast": {"temperature": 0.85, "topk": 25},      # æ’­å®¢ï¼šè‡ªç„¶æµç•…
        "dialogue": {"temperature": 0.9, "topk": 30},      # å¯¹è¯ï¼šå‡è¡¡è´¨é‡
        "narrative": {"temperature": 0.8, "topk": 20},     # å™è¿°ï¼šç¨³å®šå‡†ç¡®
        "emotional": {"temperature": 0.95, "topk": 35},    # æƒ…æ„Ÿï¼šä¸°å¯Œè¡¨è¾¾
        "technical": {"temperature": 0.75, "topk": 15}     # æŠ€æœ¯ï¼šä¸¥è°¨å‡†ç¡®
    }
    return params.get(scenario, {"temperature": 0.9, "topk": 30})

# åŠ¨æ€å‚æ•°è°ƒæ•´
audio = fireredtts2.generate_dialogue(
    text_list=text_list,
    **get_optimal_params("podcast"),
    # é«˜è´¨é‡æ¨¡å¼ï¼šé™ä½éšæœºæ€§ï¼Œæé«˜ç¨³å®šæ€§
    # temperature=0.7, topk=15  # ä¼šç‰ºç‰²ä¸€äº›åˆ›æ„æ€§
    # åˆ›æ„æ¨¡å¼ï¼šæé«˜éšæœºæ€§ï¼Œå¢åŠ å¤šæ ·æ€§
    # temperature=1.1, topk=40  # å¯èƒ½å‡ºç°ä¸ç¨³å®šæƒ…å†µ
)
```

**2. åˆ†æ®µä¼˜åŒ–ç­–ç•¥**

```python
def adaptive_generation(text_list, prompt_wav_list, prompt_text_list):
    """è‡ªé€‚åº”ç”Ÿæˆç­–ç•¥"""
    results = []
    
    for i, text in enumerate(text_list):
        # æ ¹æ®ä½ç½®è°ƒæ•´å‚æ•°
        if i == 0:  # é¦–å¥æ›´ç¨³å®š
            temp, topk = 0.8, 25
        elif i == len(text_list) - 1:  # æœ«å¥æ›´è‡ªç„¶
            temp, topk = 0.85, 28
        else:  # ä¸­é—´å¥å­æ­£å¸¸å‚æ•°
            temp, topk = 0.9, 30
        
        # æ ¹æ®å†…å®¹è°ƒæ•´
        if any(keyword in text for keyword in ["æƒ…æ„Ÿ", "æ¿€åŠ¨", "å…´å¥‹"]):
            temp += 0.05  # æƒ…æ„Ÿå†…å®¹æ›´ç”ŸåŠ¨
        
        if len(text) > 100:  # é•¿å¥å­æ›´ç¨³å®š
            temp -= 0.05
            topk -= 5
        
        # ç”Ÿæˆå•å¥
        chunk_audio = fireredtts2.generate_dialogue(
            text_list=[text],
            prompt_wav_list=prompt_wav_list,
            prompt_text_list=prompt_text_list,
            temperature=max(0.5, min(1.2, temp)),  # é™åˆ¶èŒƒå›´
            topk=max(10, min(50, topk))
        )
        results.append(chunk_audio)
    
    return torch.cat(results, dim=1)
```

#### æ€§èƒ½ä¸å†…å­˜ä¼˜åŒ–

**1. å¤§è§„æ¨¡å¯¹è¯å¤„ç†**

```python
class OptimizedFireRedTTS2:
    def __init__(self, pretrained_dir, device="cuda"):
        self.model = FireRedTTS2(pretrained_dir, "dialogue", device)
        self.max_context_length = 2500  # ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶
        
    def generate_long_dialogue(self, text_list, **kwargs):
        """é«˜æ•ˆé•¿å¯¹è¯ç”Ÿæˆ"""
        # åˆ†å—å¤„ç†ç­–ç•¥
        chunk_size = 8  # æ¯8å¥ä¸ºä¸€ç»„
        overlap_size = 2  # é‡å ä¿æŒä¸Šä¸‹æ–‡
        
        all_results = []
        context_memory = []
        
        for i in range(0, len(text_list), chunk_size - overlap_size):
            # è·å–å½“å‰å—
            end_idx = min(i + chunk_size, len(text_list))
            current_chunk = text_list[i:end_idx]
            
            # æ·»åŠ ä¸Šä¸‹æ–‡è®°å¿†
            if context_memory:
                current_chunk = context_memory[-overlap_size:] + current_chunk
            
            # ç”Ÿæˆå½“å‰å—
            chunk_result = self.model.generate_dialogue(
                text_list=current_chunk,
                **kwargs
            )
            
            # ä¿å­˜ç»“æœï¼ˆå»é™¤é‡å éƒ¨åˆ†ï¼‰
            if i > 0:
                # è®¡ç®—é‡å éƒ¨åˆ†çš„éŸ³é¢‘é•¿åº¦
                overlap_duration = self._estimate_audio_length(overlap_size)
                chunk_result = chunk_result[:, overlap_duration:]
            
            all_results.append(chunk_result)
            context_memory.extend(current_chunk)
            
            # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
            if len(context_memory) > self.max_context_length:
                context_memory = context_memory[-self.max_context_length:]
            
            # æ¸…ç†å†…å­˜
            torch.cuda.empty_cache()
        
        return torch.cat(all_results, dim=1)
    
    def _estimate_audio_length(self, text_segments):
        """ä¼°ç®—æ–‡æœ¬å¯¹åº”çš„éŸ³é¢‘é•¿åº¦"""
        avg_chars_per_second = 6  # å¹³å‡æ¯ç§’6ä¸ªå­—ç¬¦
        total_chars = sum(len(text) for text in text_segments)
        return int(total_chars / avg_chars_per_second * 24000)  # 24kHz
```

**2. å†…å­˜ç›‘æ§ä¸ç®¡ç†**

```python
import psutil
import torch
import gc

def monitor_memory_usage():
    """ç›‘æ§ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    # GPUå†…å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory
        gpu_used = torch.cuda.memory_allocated(0)
        gpu_cached = torch.cuda.memory_reserved(0)
        
        print(f"GPUå†…å­˜: {gpu_used/1e9:.2f}GB / {gpu_memory/1e9:.2f}GB")
        print(f"GPUç¼“å­˜: {gpu_cached/1e9:.2f}GB")
    
    # ç³»ç»Ÿå†…å­˜
    memory = psutil.virtual_memory()
    print(f"ç³»ç»Ÿå†…å­˜: {memory.used/1e9:.2f}GB / {memory.total/1e9:.2f}GB")
    
    return gpu_used, memory.used

def smart_memory_cleanup():
    """æ™ºèƒ½å†…å­˜æ¸…ç†"""
    gc.collect()  # Pythonåƒåœ¾å›æ”¶
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
        torch.cuda.synchronize()  # åŒæ­¥GPUæ“ä½œ

def adaptive_batch_size():
    """æ ¹æ®å†…å­˜æƒ…å†µè‡ªé€‚åº”æ‰¹å¤§å°"""
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory
        used_memory = torch.cuda.memory_allocated(0)
        free_memory = total_memory - used_memory
        
        # æ ¹æ®å‰©ä½™å†…å­˜å†³å®šæ‰¹å¤§å°
        if free_memory > 20e9:  # >20GB
            return 96
        elif free_memory > 12e9:  # >12GB
            return 48
        elif free_memory > 8e9:   # >8GB
            return 24
        else:
            return 12
    return 24  # é»˜è®¤å€¼
```

### å¤šè¯­è¨€æ”¯æŒè¯¦è§£

#### æ”¯æŒçš„è¯­è¨€
- ä¸­æ–‡ï¼ˆç®€ä½“/ç¹ä½“ï¼‰
- è‹±æ–‡
- æ—¥æ–‡
- éŸ©æ–‡
- æ³•æ–‡
- å¾·æ–‡
- ä¿„æ–‡

#### å¤šè¯­è¨€æ··åˆç¤ºä¾‹

```python
text_list = [
    "[S1]Hello, ä½ å¥½ï¼ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
    "[S2]å¤©æ°—ã¯ã¨ã¦ã‚‚ã„ã„ã§ã™ã€‚Comment allez-vous?",
    "[S1]I'm doing great! æˆ‘ä»¬ä¸€èµ·å­¦ä¹ å¤šè¯­è¨€TTSå§ã€‚"
]
```

### æ•…éšœæ’é™¤ä¸æ€§èƒ½è°ƒä¼˜

#### å¸¸è§é—®é¢˜æ·±åº¦è§£å†³

**1. CUDAå†…å­˜é—®é¢˜**

```bash
# é—®é¢˜è¯Šæ–­
nvidia-smi  # æŸ¥çœ‹å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ

# è§£å†³æ–¹æ¡ˆ 1ï¼šç¯å¢ƒå˜é‡è®¾ç½®
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
export CUDA_LAUNCH_BLOCKING=1

# è§£å†³æ–¹æ¡ˆ 2ï¼šä»£ç çº§åˆ«ä¼˜åŒ–
```

```python
# åŠ¨æ€è°ƒæ•´æ‰¹å¤§å°
def get_optimal_batch_size():
    try:
        torch.cuda.empty_cache()
        total_memory = torch.cuda.get_device_properties(0).total_memory
        current_memory = torch.cuda.memory_allocated(0)
        available_memory = total_memory - current_memory
        
        # æ ¹æ®å¯ç”¨å†…å­˜è®¡ç®—æœ€ä¼˜æ‰¹å¤§å°
        if available_memory > 20e9:    # >20GB
            return 64
        elif available_memory > 12e9:  # >12GB
            return 32
        elif available_memory > 8e9:   # >8GB
            return 16
        else:
            return 8
    except:
        return 8  # ä¿å®ˆé»˜è®¤å€¼

# å†…å­˜ç›‘æ§è£…é¥°å™¨
def memory_monitor(func):
    def wrapper(*args, **kwargs):
        torch.cuda.empty_cache()
        start_memory = torch.cuda.memory_allocated()
        
        result = func(*args, **kwargs)
        
        end_memory = torch.cuda.memory_allocated()
        print(f"å†…å­˜å¢é•¿: {(end_memory - start_memory) / 1e9:.2f}GB")
        
        return result
    return wrapper
```

**2. æ¨¡å‹åŠ è½½é—®é¢˜**

```python
# é€æ­¥åŠ è½½ç­–ç•¥
def load_model_safely(pretrained_dir, device="cuda"):
    try:
        print("æ­£åœ¨åŠ è½½LLMæ¨¡å—...")
        # å…ˆåŠ è½½é…ç½®
        llm_config_path = os.path.join(pretrained_dir, "config_llm.json")
        if not os.path.exists(llm_config_path):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶: {llm_config_path}")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        required_files = [
            "llm_posttrain.pt", "codec.pt", 
            "config_codec.json", "Qwen2.5-1.5B"
        ]
        
        missing_files = []
        for file in required_files:
            if not os.path.exists(os.path.join(pretrained_dir, file)):
                missing_files.append(file)
        
        if missing_files:
            print(f"ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            print("è¯·æ£€æŸ¥æ¨¡å‹ä¸‹è½½æ˜¯å¦å®Œæ•´")
            return None
        
        # åˆ†æ­¥åŠ è½½
        model = FireRedTTS2(pretrained_dir, "dialogue", device)
        print("æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model
        
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        # æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
        import traceback
        traceback.print_exc()
        return None

# é‡è¯•æœºåˆ¶
def retry_model_loading(pretrained_dir, max_retries=3):
    for attempt in range(max_retries):
        print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•åŠ è½½æ¨¡å‹...")
        model = load_model_safely(pretrained_dir)
        if model is not None:
            return model
        
        # æ¸…ç†å†…å­˜åé‡è¯•
        torch.cuda.empty_cache()
        time.sleep(2)
    
    raise RuntimeError("æ¨¡å‹åŠ è½½å¤šæ¬¡å¤±è´¥")
```

**3. éŸ³é¢‘è´¨é‡é—®é¢˜è¯Šæ–­**

```python
def diagnose_audio_quality(audio_path, reference_text):
    """è¯Šæ–­éŸ³é¢‘è´¨é‡é—®é¢˜"""
    import librosa
    import numpy as np
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=None)
    
    # æ£€æŸ¥åŸºæœ¬å‚æ•°
    duration = len(audio) / sr
    print(f"éŸ³é¢‘æ—¶é•¿: {duration:.2f}ç§’")
    print(f"é‡‡æ ·ç‡: {sr}Hz")
    print(f"å£°é“æ•°: {1 if audio.ndim == 1 else audio.shape[0]}")
    
    # æ£€æŸ¥éŸ³é¢‘è´¨é‡
    # 1. éŸ³é‡æ£€æŸ¥
    rms_energy = np.sqrt(np.mean(audio**2))
    print(f"RMSèƒ½é‡: {rms_energy:.4f}")
    
    if rms_energy < 0.01:
        print("âš ï¸ è­¦å‘Šï¼šéŸ³é¢‘éŸ³é‡è¿‡ä½")
    elif rms_energy > 0.5:
        print("âš ï¸ è­¦å‘Šï¼šéŸ³é¢‘éŸ³é‡è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨çˆ†éŸ³")
    
    # 2. é¢‘è°±æ£€æŸ¥
    stft = librosa.stft(audio)
    magnitude = np.abs(stft)
    
    # æ£€æŸ¥é«˜é¢‘èƒ½é‡
    high_freq_energy = np.mean(magnitude[magnitude.shape[0]//2:, :])
    total_energy = np.mean(magnitude)
    high_freq_ratio = high_freq_energy / total_energy
    
    if high_freq_ratio < 0.1:
        print("âš ï¸ è­¦å‘Šï¼šç¼ºä¹é«˜é¢‘ä¿¡æ¯ï¼Œå¯èƒ½å½±å“è¯­éŸ³æ¸…æ™°åº¦")
    
    # 3. é™éŸ³æ£€æŸ¥
    silence_threshold = 0.01
    silence_frames = np.sum(np.abs(audio) < silence_threshold)
    silence_ratio = silence_frames / len(audio)
    
    if silence_ratio > 0.3:
        print(f"âš ï¸ è­¦å‘Šï¼šéŸ³é¢‘ä¸­åŒ…å« {silence_ratio:.1%} çš„é™éŸ³")
    
    # 4. æ—¶é•¿æ£€æŸ¥
    if duration < 3:
        print("âš ï¸ è­¦å‘Šï¼šéŸ³é¢‘æ—¶é•¿è¿‡çŸ­ï¼ˆ<3ç§’ï¼‰ï¼Œå¯èƒ½å½±å“å…‹éš†æ•ˆæœ")
    elif duration > 30:
        print("âš ï¸ è­¦å‘Šï¼šéŸ³é¢‘æ—¶é•¿è¿‡é•¿ï¼ˆ>30ç§’ï¼‰ï¼Œå»ºè®®æˆªå–ç²¾åéƒ¨åˆ†")
    
    return {
        "duration": duration,
        "sample_rate": sr,
        "rms_energy": rms_energy,
        "high_freq_ratio": high_freq_ratio,
        "silence_ratio": silence_ratio
    }

# éŸ³é¢‘é¢„å¤„ç†å»ºè®®
def preprocess_audio_recommendations(diagnosis_result):
    recommendations = []
    
    if diagnosis_result["rms_energy"] < 0.01:
        recommendations.append("å»ºè®®å¢åŠ éŸ³é¢‘éŸ³é‡")
    
    if diagnosis_result["sample_rate"] not in [16000, 24000, 44100, 48000]:
        recommendations.append(f"å»ºè®®é‡é‡‡æ ·åˆ°16kHzæˆ–24kHz")
    
    if diagnosis_result["high_freq_ratio"] < 0.1:
        recommendations.append("å»ºè®®æ£€æŸ¥éŸ³é¢‘è´¨é‡ï¼Œå¯èƒ½éœ€è¦æ›´é«˜è´¨é‡çš„å½•éŸ³")
    
    if diagnosis_result["silence_ratio"] > 0.3:
        recommendations.append("å»ºè®®ç§»é™¤è¿‡å¤šçš„é™éŸ³æ®µ")
    
    return recommendations
```

**4. æ€§èƒ½ä¼˜åŒ–å»ºè®®**

```python
class PerformanceProfiler:
    def __init__(self):
        self.timings = {}
        
    def profile_generation(self, text_list, **kwargs):
        import time
        
        # åˆ†æ­¥è®¡æ—¶
        start_time = time.time()
        
        # æ¨¡å‹åˆå§‹åŒ–æ—¶é—´
        init_start = time.time()
        model = FireRedTTS2("./pretrained_models/FireRedTTS2", "dialogue", "cuda")
        self.timings["model_init"] = time.time() - init_start
        
        # æ–‡æœ¬é¢„å¤„ç†æ—¶é—´
        preprocess_start = time.time()
        # æ¨¡æ‹Ÿé¢„å¤„ç†è¿‡ç¨‹
        self.timings["preprocessing"] = time.time() - preprocess_start
        
        # ç”Ÿæˆæ—¶é—´
        generation_start = time.time()
        result = model.generate_dialogue(text_list, **kwargs)
        self.timings["generation"] = time.time() - generation_start
        
        # æ€»æ—¶é—´
        self.timings["total"] = time.time() - start_time
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        total_chars = sum(len(text) for text in text_list)
        audio_duration = result.shape[1] / 24000  # 24kHz
        
        self.timings["chars_per_second"] = total_chars / self.timings["generation"]
        self.timings["rtf"] = self.timings["generation"] / audio_duration  # Real-Time Factor
        
        return result, self.timings
    
    def print_performance_report(self):
        print("\nğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print("=" * 50)
        print(f"æ¨¡å‹åˆå§‹åŒ–: {self.timings.get('model_init', 0):.2f}s")
        print(f"æ–‡æœ¬é¢„å¤„ç†: {self.timings.get('preprocessing', 0):.2f}s")
        print(f"éŸ³é¢‘ç”Ÿæˆ: {self.timings.get('generation', 0):.2f}s")
        print(f"æ€»è€—æ—¶: {self.timings.get('total', 0):.2f}s")
        print(f"å¤„ç†é€Ÿåº¦: {self.timings.get('chars_per_second', 0):.1f} å­—ç¬¦/ç§’")
        print(f"RTFç³»æ•°: {self.timings.get('rtf', 0):.2f}x")
        
        # æ€§èƒ½å»ºè®®
        if self.timings.get('rtf', 0) > 0.5:
            print("âš ï¸ å»ºè®®ï¼šRTFç³»æ•°è¿‡é«˜ï¼Œè€ƒè™‘ä¼˜åŒ–æ¨¡å‹æˆ–ç¡¬ä»¶")
        
        if self.timings.get('chars_per_second', 0) < 10:
            print("âš ï¸ å»ºè®®ï¼šå¤„ç†é€Ÿåº¦è¿‡æ…¢ï¼Œæ£€æŸ¥GPUæ€§èƒ½")
```

#### æ€§èƒ½åŸºå‡†

åœ¨ L20 GPU ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼š
- é¦–åŒ…å»¶è¿Ÿï¼šâ‰¤140ms
- æœ€å¤§å¯¹è¯é•¿åº¦ï¼š3åˆ†é’Ÿ+
- æ”¯æŒè¯´è¯äººæ•°ï¼šæœ€å¤š4äºº
- éŸ³é¢‘é‡‡æ ·ç‡ï¼š24kHz

## ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„è®¾è®¡

FireRedTTS2 é‡‡ç”¨**åŒTransformeræ¶æ„**ï¼Œå¤„ç†æ–‡æœ¬ä¸è¯­éŸ³äº¤é”™åºåˆ—ï¼ˆtext-speech interleaved sequenceï¼‰ï¼Œå®ç°ç«¯åˆ°ç«¯æµå¼è¯­éŸ³ç”Ÿæˆã€‚

#### ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "è¾“å…¥å±‚"
        A[æ–‡æœ¬è¾“å…¥] --> B[æ–‡æœ¬é¢„å¤„ç†]
        C[å‚è€ƒéŸ³é¢‘] --> D[éŸ³é¢‘é¢„å¤„ç†]
        E[è¯´è¯äººæ ‡è®°] --> F[æ ‡è®°å¤„ç†]
    end
    
    subgraph "ç¼–ç å±‚"
        B --> G[Qwen2.5æ–‡æœ¬Tokenizer]
        D --> H[WhisperéŸ³é¢‘ç¼–ç å™¨]
        F --> I[è¯´è¯äººåµŒå…¥]
    end
    
    subgraph "æ ¸å¿ƒå¤„ç†å±‚ - åŒTransformeræ¶æ„"
        G --> J[LLM Backbone]
        H --> K[è¯­éŸ³ç¼–ç ]
        I --> J
        J --> L[æ–‡æœ¬-è¯­éŸ³äº¤é”™åºåˆ—]
        L --> M[Transformer Decoder]
        M --> N[æ®‹å·®å‘é‡é‡åŒ– RVQ]
    end
    
    subgraph "è§£ç å±‚"
        N --> O[Vocoså£°å­¦è§£ç å™¨]
        O --> P[ä¸Šé‡‡æ ·ç½‘ç»œ]
        P --> Q[æœ€ç»ˆéŸ³é¢‘é‡å»º]
    end
    
    subgraph "è¾“å‡ºå±‚"
        Q --> R[24kHzé«˜è´¨é‡éŸ³é¢‘]
    end
```

#### æŠ€æœ¯æ ˆæ¶æ„

```mermaid
graph LR
    subgraph "å‰ç«¯ç•Œé¢å±‚"
        A[Gradio Web UI]
    end
    
    subgraph "APIæ¥å£å±‚"
        B[FireRedTTS2 API]
        C[generate_dialogue]
        D[generate_monologue]
    end
    
    subgraph "æ¨¡å‹å±‚"
        E[LLMæ¨¡å— - Qwen2.5åŸºç¡€]
        F[Codecæ¨¡å— - Vocosæ¶æ„]
        G[åŒTransformerå¤„ç†å™¨]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚"
        H[PyTorch 2.7.1]
        I[CUDA 12.6]
        J[Transformers]
        K[TorchAudio]
    end
    
    A --> B
    B --> C
    B --> D
    C --> E
    D --> E
    E --> G
    F --> G
    G --> H
    H --> I
    H --> J
    H --> K
```

### æ ¸å¿ƒæ¨¡å—è¯¦è§£

#### 1. LLMæ¨¡å— (`fireredtts2/llm/`) - å¤§è¯­è¨€æ¨¡å‹æ ¸å¿ƒ

**åŠŸèƒ½æ¶æ„**ï¼š
- **åŒTransformerè®¾è®¡**ï¼šé‡‡ç”¨Backbone + Decoderæ¶æ„
- **Backbone**ï¼šåŸºäºQwen-3Bï¼Œè´Ÿè´£ä¸»è¦çš„è¯­è¨€ç†è§£
- **Decoder**ï¼šåŸºäºQwen-500Mï¼Œä¸“é—¨å¤„ç†éŸ³é¢‘ç æœ¬ç”Ÿæˆ

**æ ¸å¿ƒæŠ€æœ¯å®ç°**ï¼š
```python
# æ¨¡å‹é…ç½®å‚æ•°
class ModelArgs:
    backbone_flavor: str = "qwen-3b"     # ä¸»å¹²ç½‘ç»œ
    decoder_flavor: str = "qwen-500m"    # è§£ç å™¨ç½‘ç»œ
    text_vocab_size: int = 128256        # æ–‡æœ¬è¯æ±‡è¡¨å¤§å°
    audio_vocab_size: int = 2051         # éŸ³é¢‘è¯æ±‡è¡¨å¤§å°
    audio_num_codebooks: int = 32        # éŸ³é¢‘ç æœ¬æ•°é‡
```

**æŠ€æœ¯ç‰¹è‰²**ï¼š
- **æ–‡æœ¬-éŸ³é¢‘è”åˆå»ºæ¨¡**ï¼šé€šè¿‡17ç»´tokenå¤„ç†æ–‡æœ¬å’ŒéŸ³é¢‘
- **KVç¼“å­˜æœºåˆ¶**ï¼šæ”¯æŒæµå¼æ¨ç†ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦
- **å¤šç æœ¬é¢„æµ‹**ï¼šæ¸è¿›å¼ç”Ÿæˆ32å±‚éŸ³é¢‘ç æœ¬
- **å› æœæ³¨æ„åŠ›æœºåˆ¶**ï¼šç¡®ä¿ç”Ÿæˆçš„è‡ªå›å½’ç‰¹æ€§

#### 2. Codecæ¨¡å— (`fireredtts2/codec/`) - éŸ³é¢‘ç¼–è§£ç æ ¸å¿ƒ

**æ ¸å¿ƒæ¶æ„ç»„ä»¶**ï¼š

**æ®‹å·®å‘é‡é‡åŒ– (RVQ)**ï¼š
- **åŠŸèƒ½**ï¼šå°†è¿ç»­éŸ³é¢‘ç‰¹å¾ç¦»æ•£åŒ–ä¸ºtokenåºåˆ—
- **å±‚æ¬¡ç»“æ„**ï¼š32å±‚ç æœ¬ï¼Œé€çº§ç»†åŒ–éŸ³é¢‘è¡¨ç¤º
- **æŠ€æœ¯ä¼˜åŠ¿**ï¼šé€šè¿‡æ®‹å·®å­¦ä¹ æé«˜é‡å»ºè´¨é‡

```python
class ResidualVQ:
    def __init__(self, 
                 num_quantizers: int = 32,     # é‡åŒ–å™¨æ•°é‡
                 codebook_size: int = 2048,    # ç æœ¬å¤§å°
                 codebook_dim: int = 256):     # ç æœ¬ç»´åº¦
```

**Whisperé£æ ¼ç¼–ç å™¨**ï¼š
- **SSLç¼–ç å™¨**ï¼šåˆ©ç”¨é¢„è®­ç»ƒçš„Whisperæå–è¯­ä¹‰ç‰¹å¾
- **å£°å­¦ç¼–ç å™¨**ï¼šä¸“é—¨å¤„ç†éŸ³é¢‘çš„å£°å­¦ç‰¹æ€§
- **ç‰¹å¾èåˆ**ï¼šå°†è¯­ä¹‰å’Œå£°å­¦ç‰¹å¾ç»“åˆ

**Vocoså£°å­¦è§£ç å™¨**ï¼š
- **å› æœå·ç§¯è®¾è®¡**ï¼šæ”¯æŒæµå¼è§£ç 
- **Transformeréª¨å¹²**ï¼šæä¾›å¼ºå¤§çš„åºåˆ—å»ºæ¨¡èƒ½åŠ›
- **åˆ†å—å¤„ç†**ï¼šæ”¯æŒå®æ—¶éŸ³é¢‘ç”Ÿæˆ

#### 3. æµå¼å¤„ç†å¼•æ“ (`fireredtts2/fireredtts2.py`)

**æ ¸å¿ƒæ¥å£è®¾è®¡**ï¼š

```python
class FireRedTTS2:
    def generate_dialogue(self, 
                         text_list,           # å¯¹è¯æ–‡æœ¬åºåˆ—
                         prompt_wav_list,     # å‚è€ƒéŸ³é¢‘åˆ—è¡¨
                         prompt_text_list,    # å‚è€ƒæ–‡æœ¬åˆ—è¡¨
                         temperature=0.9,     # ç”Ÿæˆéšæœºæ€§
                         topk=30):           # é‡‡æ ·èŒƒå›´
```

**æŠ€æœ¯å®ç°ç»†èŠ‚**ï¼š
- **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šç»´æŠ¤å¯¹è¯å†å²å’Œè¯´è¯äººçŠ¶æ€
- **å†…å­˜ä¼˜åŒ–**ï¼š16kHzå†…éƒ¨å¤„ç†ï¼Œ24kHzè¾“å‡º
- **æ‰¹å¤„ç†æ”¯æŒ**ï¼š6ç§’éŸ³é¢‘å—å¤„ç†ï¼Œæé«˜æ•ˆç‡

### æ•°æ®æµå¤„ç†æ¶æ„

#### è¯¦ç»†å¤„ç†æµæ°´çº¿

```mermaid
graph TB
    subgraph "è¾“å…¥é¢„å¤„ç†é˜¶æ®µ"
        A1[åŸå§‹æ–‡æœ¬] --> A2[ç¬¦å·æ ‡å‡†åŒ–]
        A2 --> A3[è¯­è¨€æ£€æµ‹]
        A3 --> A4[æ™ºèƒ½åˆ†å‰²]
        A4 --> A5[è¯´è¯äººæ ‡è®°]
        
        B1[å‚è€ƒéŸ³é¢‘] --> B2[é‡é‡‡æ ·åˆ°16kHz]
        B2 --> B3[Whisperç‰¹å¾æå–]
        B3 --> B4[å£°å­¦ç‰¹å¾ç¼–ç ]
    end
    
    subgraph "TokenåŒ–é˜¶æ®µ"
        A5 --> C1[Qwen2.5æ–‡æœ¬ç¼–ç ]
        B4 --> C2[RVQéŸ³é¢‘ç¼–ç ]
        C1 --> C3[17ç»´Tokenåºåˆ—]
        C2 --> C3
    end
    
    subgraph "æ ¸å¿ƒç”Ÿæˆé˜¶æ®µ"
        C3 --> D1[Backbone Transformer]
        D1 --> D2[æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—]
        D2 --> D3[ä¸Šä¸‹æ–‡èåˆ]
        D3 --> D4[Decoder Transformer]
        D4 --> D5[32å±‚ç æœ¬é€çº§ç”Ÿæˆ]
    end
    
    subgraph "éŸ³é¢‘é‡å»ºé˜¶æ®µ"
        D5 --> E1[RVQè§£ç ]
        E1 --> E2[Vocoså£°å­¦è§£ç ]
        E2 --> E3[ä¸Šé‡‡æ ·åˆ°24kHz]
        E3 --> E4[æœ€ç»ˆéŸ³é¢‘è¾“å‡º]
    end
```

#### å…³é”®æŠ€æœ¯èŠ‚ç‚¹è¯¦è§£

**1. Tokenäº¤é”™åºåˆ—å¤„ç†**ï¼š
```python
# æ¯ä¸ªæ—¶é—´æ­¥çš„tokenè¡¨ç¤ºï¼š[audio_tokens(16ç»´) + text_token(1ç»´)]
tokens.shape = (batch_size, seq_len, 17)
# å‰16ç»´ï¼šéŸ³é¢‘ç æœ¬tokensï¼Œç¬¬17ç»´ï¼šæ–‡æœ¬token
```

**2. å› æœæ©ç æœºåˆ¶**ï¼š
- **Backboneæ©ç **ï¼šç¡®ä¿åªèƒ½çœ‹åˆ°å†å²ä¿¡æ¯
- **Decoderæ©ç **ï¼šç æœ¬é—´çš„å±‚æ¬¡ä¾èµ–å…³ç³»
- **åŠ¨æ€æ©ç **ï¼šæ ¹æ®è¾“å…¥ä½ç½®åŠ¨æ€è°ƒæ•´

**3. ç¼“å­˜ä¼˜åŒ–ç­–ç•¥**ï¼š
- **KVç¼“å­˜**ï¼šé¿å…é‡å¤è®¡ç®—æ³¨æ„åŠ›æƒé‡
- **éŸ³é¢‘ç¼“å­˜**ï¼šæµå¼è§£ç ä¸­çš„çŠ¶æ€ä¿æŒ
- **å†…å­˜ç®¡ç†**ï¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜

### å…³é”®æŠ€æœ¯æ·±åº¦å‰–æ

#### 1. åŒTransformerååŒæœºåˆ¶

**Backbone-Decoderåä½œæµç¨‹**ï¼š
```python
# Backboneå¤„ç†ï¼šæ–‡æœ¬-éŸ³é¢‘è”åˆç†è§£
backbone_output = self.backbone(
    combined_embeddings,     # æ–‡æœ¬+éŸ³é¢‘åµŒå…¥
    causal_mask=causal_mask  # å› æœæ³¨æ„åŠ›æ©ç 
)

# Decoderå¤„ç†ï¼šé€å±‚ç æœ¬ç”Ÿæˆ
for i in range(self.audio_num_codebooks):
    decoder_output = self.decoder(
        projected_features,
        causal_mask=decoder_mask
    )
    codebook_logits = self.audio_head[i] @ decoder_output
    sampled_token = sample_topk(codebook_logits, topk, temperature)
```

**æŠ€æœ¯ä¼˜åŠ¿**ï¼š
- **ä¸“ä¸šåˆ†å·¥**ï¼šBackboneè´Ÿè´£ç†è§£ï¼ŒDecoderè´Ÿè´£ç”Ÿæˆ
- **æ¢¯åº¦è§£è€¦**ï¼šä¸åŒæ¨¡å—å¯ç‹¬ç«‹ä¼˜åŒ–
- **æ¨ç†æ•ˆç‡**ï¼šå¹¶è¡Œå¤„ç†æé«˜ç”Ÿæˆé€Ÿåº¦

#### 2. æ®‹å·®å‘é‡é‡åŒ– (RVQ) æ ¸å¿ƒç®—æ³•

**å¤šå±‚æ¬¡é‡åŒ–ç­–ç•¥**ï¼š
```python
def encode_codes(self, z: torch.Tensor):
    residual = z.clone()
    all_indices = []
    
    # 32å±‚é€çº§é‡åŒ–
    for quantizer in self.quantizers:
        quantized, indices = quantizer.encode_code(residual)
        residual = residual - quantized  # æ®‹å·®ä¼ é€’
        all_indices.append(indices)
    
    return torch.stack(all_indices)  # (32, B, T)
```

**æŠ€æœ¯çªç ´**ï¼š
- **ä¿¡æ¯æ— æŸ**ï¼šé€šè¿‡æ®‹å·®æœºåˆ¶æœ€å¤§åŒ–ä¿ç•™éŸ³é¢‘ç»†èŠ‚
- **åˆ†å±‚è¡¨ç¤º**ï¼šä½å±‚ç æœ¬æ•è·åŸºç¡€ç‰¹å¾ï¼Œé«˜å±‚ç æœ¬ç»†åŒ–ç»†èŠ‚
- **å‹ç¼©æ•ˆç‡**ï¼š2048å¤§å°ç æœ¬å®ç°é«˜æ•ˆå‹ç¼©

#### 3. æµå¼ç”Ÿæˆæ ¸å¿ƒç®—æ³•

**å®æ—¶ç”Ÿæˆæµæ°´çº¿**ï¼š
```python
def generate_frame(self, tokens, mask, input_pos, temperature, topk):
    # 1. Backboneæ¨ç†
    h = self.backbone(tokens, input_pos, mask)
    
    # 2. ç¬¬ä¸€ä¸ªç æœ¬é‡‡æ ·
    c0_logits = self.codebook0_head(h[:, -1, :])
    c0_sample = sample_topk(c0_logits, topk, temperature)
    
    # 3. è§£ç å™¨é€å±‚ç”Ÿæˆ
    self.decoder.reset_caches()  # é‡ç½®è§£ç å™¨ç¼“å­˜
    for i in range(1, self.audio_num_codebooks):
        decoder_h = self.decoder(projected_h, input_pos, decoder_mask)
        ci_logits = self.audio_head[i-1] @ decoder_h[:, -1, :]
        ci_sample = sample_topk(ci_logits, 10, 0.75)  # å›ºå®šå‚æ•°ä¼˜åŒ–
        
    return combined_samples  # (B, 32)
```

**å»¶è¿Ÿä¼˜åŒ–æŠ€æœ¯**ï¼š
- **é¦–åŒ…å»¶è¿Ÿ**ï¼š140mså†…å®Œæˆç¬¬ä¸€ä¸ªéŸ³é¢‘å—ç”Ÿæˆ
- **å¹¶è¡Œè§£ç **ï¼šç æœ¬ç”Ÿæˆä¸éŸ³é¢‘è§£ç å¹¶è¡Œè¿›è¡Œ
- **é¢„æµ‹ç¼“å­˜**ï¼šé¢„æµ‹ä¸‹ä¸€å¸§éœ€è¦çš„è®¡ç®—ç»“æœ

#### 4. é›¶æ ·æœ¬è¯­éŸ³å…‹éš†æœºåˆ¶

**è·¨è¯­è¨€éŸ³è‰²è¿ç§»åŸç†**ï¼š
```python
def prepare_prompt(self, text, speaker, audio_path):
    # æå–å‚è€ƒéŸ³é¢‘ç‰¹å¾
    audio_tensor = self.load_prompt_audio(audio_path)  # 16kHz
    
    # æ„å»ºæç¤ºæ®µ
    return Segment(
        text=text,      # å‚è€ƒæ–‡æœ¬ï¼ˆå¯è·¨è¯­è¨€ï¼‰
        speaker=speaker, # è¯´è¯äººæ ‡è¯†
        audio=audio_tensor  # å‚è€ƒéŸ³é¢‘ç‰¹å¾
    )
```

**æŠ€æœ¯åˆ›æ–°**ï¼š
- **éŸ³è‰²è§£è€¦**ï¼šåˆ†ç¦»è¯­è¨€å†…å®¹ä¸è¯´è¯äººç‰¹å¾
- **ç‰¹å¾å¯¹é½**ï¼šä¸åŒè¯­è¨€é—´çš„éŸ³è‰²ç‰¹å¾å¯¹é½
- **å°‘æ ·æœ¬å­¦ä¹ **ï¼š3-10ç§’éŸ³é¢‘å³å¯å®ç°å…‹éš†

#### 5. å¤šè¯´è¯äººå¯¹è¯ç®¡ç†

**çŠ¶æ€ç»´æŠ¤æœºåˆ¶**ï¼š
```python
class DialogueManager:
    def __init__(self):
        self.speaker_history = {}    # è¯´è¯äººå†å²
        self.context_memory = []     # ä¸Šä¸‹æ–‡è®°å¿†
        self.emotion_state = {}      # æƒ…æ„ŸçŠ¶æ€
    
    def update_speaker_context(self, speaker_id, new_segment):
        # æ›´æ–°è¯´è¯äººç‰¹å®šçš„ä¸Šä¸‹æ–‡
        self.speaker_history[speaker_id].append(new_segment)
        
        # ç»´æŠ¤å…¨å±€å¯¹è¯æµ
        self.context_memory.append((speaker_id, new_segment))
```

**ä¸€è‡´æ€§ä¿è¯**ï¼š
- **éŸ³è‰²è¿ç»­æ€§**ï¼šåŒä¸€è¯´è¯äººåœ¨å¯¹è¯ä¸­ä¿æŒéŸ³è‰²ç¨³å®š
- **æƒ…æ„Ÿä¼ é€’**ï¼šè€ƒè™‘ä¸Šä¸‹æ–‡æƒ…æ„ŸçŠ¶æ€
- **éŸµå¾‹åè°ƒ**ï¼šä¸åŒè¯´è¯äººé—´çš„è‡ªç„¶è¿‡æ¸¡

### æ€§èƒ½ä¼˜åŒ–ä¸æ‰©å±•æ€§è®¾è®¡

#### 1. è®¡ç®—ä¼˜åŒ–ç­–ç•¥

**å†…å­˜ç®¡ç†**ï¼š
```python
# åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–
CHUNK_SIZE = 6 * 16000  # 6ç§’éŸ³é¢‘å—
MAX_GENERATION_LEN = int(max_audio_length_ms / 80)  # åŸºäºæ—¶é•¿è®¡ç®—

# ä¸Šä¸‹æ–‡é•¿åº¦ç®¡ç†
max_context_len = max_seq_len - max_generation_len
if curr_tokens.size(1) >= max_context_len:
    raise ValueError(f"è¾“å…¥è¿‡é•¿ï¼Œå¿…é¡»å°äº {max_context_len}")
```

**æ¨ç†åŠ é€Ÿ**ï¼š
- **æ··åˆç²¾åº¦**ï¼šFP16æ¨ç†å‡å°‘æ˜¾å­˜å ç”¨
- **åŠ¨æ€å›¾ä¼˜åŒ–**ï¼štorch.compileåŠ é€Ÿ
- **æ‰¹å¤„ç†ç­–ç•¥**ï¼šè‡ªé€‚åº”æ‰¹å¤§å°è°ƒæ•´

#### 2. æ¨¡å—åŒ–æ‰©å±•æ¶æ„

**æ’ä»¶åŒ–è®¾è®¡**ï¼š
```python
class ExtensibleFireRedTTS2(FireRedTTS2):
    def __init__(self, pretrained_dir, plugins=None):
        super().__init__(pretrained_dir)
        self.plugins = plugins or []
        
    def register_plugin(self, plugin):
        """æ³¨å†ŒåŠŸèƒ½æ’ä»¶"""
        self.plugins.append(plugin)
        
    def generate_with_plugins(self, *args, **kwargs):
        """æ”¯æŒæ’ä»¶çš„ç”Ÿæˆæ¥å£"""
        for plugin in self.plugins:
            kwargs = plugin.preprocess(kwargs)
        
        result = self.generate(*args, **kwargs)
        
        for plugin in self.plugins:
            result = plugin.postprocess(result)
        
        return result
```

**å¯æ‰©å±•ç»„ä»¶**ï¼š
- **æƒ…æ„Ÿæ§åˆ¶æ’ä»¶**ï¼šç»†ç²’åº¦æƒ…æ„Ÿè¡¨è¾¾æ§åˆ¶
- **éŸ³è‰²åº“ç®¡ç†**ï¼šåŠ¨æ€éŸ³è‰²åŠ è½½ä¸åˆ‡æ¢
- **å®æ—¶ç‰¹æ•ˆ**ï¼šéŸ³é¢‘åå¤„ç†æ•ˆæœ

#### 3. é…ç½®ç®¡ç†ç³»ç»Ÿ

**åˆ†å±‚é…ç½®ç»“æ„**ï¼š
```json
{
  "llm_models": {
    "backbone_flavor": "qwen-3b",
    "decoder_flavor": "qwen-500m",
    "text_vocab_size": 128256,
    "audio_vocab_size": 2051,
    "audio_num_codebooks": 32,
    "decoder_loss_weight": 0.5
  },
  "codec": {
    "ssl_adaptor": {"embed_dim": 1024, "num_layers": 6},
    "rvq": {"num_quantizers": 32, "codebook_size": 2048},
    "acoustic_decoder": {"embed_dim": 1024, "num_layers": 12}
  },
  "generation": {
    "temperature": 0.9,
    "topk": 30,
    "max_length": 3000
  }
}
```

#### 4. åˆ†å¸ƒå¼éƒ¨ç½²æ”¯æŒ

**æ¨¡å‹å¹¶è¡Œç­–ç•¥**ï¼š
- **æµæ°´çº¿å¹¶è¡Œ**ï¼šLLMå’ŒCodecåˆ†åˆ«éƒ¨ç½²
- **æ•°æ®å¹¶è¡Œ**ï¼šå¤šGPUåŒæ—¶å¤„ç†ä¸åŒè¯·æ±‚
- **æ··åˆå¹¶è¡Œ**ï¼šç»“åˆæµæ°´çº¿å’Œæ•°æ®å¹¶è¡Œ

**æœåŠ¡åŒ–æ¶æ„**ï¼š
```python
class FireRedTTSService:
    def __init__(self, config):
        self.llm_service = LLMService(config.llm_config)
        self.codec_service = CodecService(config.codec_config)
        self.load_balancer = LoadBalancer()
    
    async def generate_async(self, request):
        """å¼‚æ­¥ç”Ÿæˆæ¥å£"""
        llm_result = await self.llm_service.process(request)
        audio_result = await self.codec_service.decode(llm_result)
        return audio_result
```

---

## ä½¿ç”¨å£°æ˜ âš ï¸

### å­¦æœ¯ç ”ç©¶ç”¨é€”
- æœ¬é¡¹ç›®åŒ…å«é›¶æ ·æœ¬è¯­éŸ³å…‹éš†åŠŸèƒ½ï¼Œ**ä»…é™å­¦æœ¯ç ”ç©¶ç”¨é€”**
- è¯·å‹¿ç”¨äºä»»ä½•éæ³•æ´»åŠ¨
- å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•æ»¥ç”¨è´£ä»»

### ç‰¹æ®ŠéŸ³è‰²æˆæƒ
- ç‰¹å®šå£°éŸ³ï¼ˆå¦‚"è‚¥æ°""æƒ å­"ï¼‰æ¥è‡ªæ’­å®¢"è‚¥è¯è¿ç¯‡"
- **æœªç»æˆæƒä¸å¾—å•†ä¸šä½¿ç”¨**

### æŠ¥å‘Šæ»¥ç”¨
å¦‚å‘ç°ä»»ä½•**æ»¥ç”¨**ã€**è¯¯ç”¨**æˆ–**æ¬ºè¯ˆ**è¡Œä¸ºï¼Œè¯·ç«‹å³å‘å›¢é˜Ÿä¸¾æŠ¥ã€‚

---

*æ›´å¤šä¿¡æ¯è¯·å‚è€ƒï¼š*
- [æŠ€æœ¯è®ºæ–‡](https://arxiv.org/abs/2509.02020)
- [æ¼”ç¤ºé¡µé¢](https://fireredteam.github.io/demos/firered_tts_2/)
- [Hugging Faceæ¨¡å‹é¡µé¢](https://huggingface.co/FireRedTeam/FireRedTTS2)
- [GitHubé¡¹ç›®ä¸»é¡µ](https://github.com/FireRedTeam/FireRedTTS2)