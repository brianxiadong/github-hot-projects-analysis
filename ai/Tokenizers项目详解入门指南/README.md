# Tokenizers é¡¹ç›®è¯¦è§£å…¥é—¨æŒ‡å—ï¼šä»é›¶å¼€å§‹æŒæ¡ç°ä»£åˆ†è¯æŠ€æœ¯

<p align="center">
    <img src="https://huggingface.co/landing/assets/tokenizers/tokenizers-logo.png" width="600"/>
</p>

<p align="center">
    <img alt="Build" src="https://github.com/huggingface/tokenizers/workflows/Rust/badge.svg">
    <a href="https://github.com/huggingface/tokenizers/blob/main/LICENSE">
        <img alt="GitHub" src="https://img.shields.io/github/license/huggingface/tokenizers.svg?color=blue&cachedrop">
    </a>
    <a href="https://pepy.tech/project/tokenizers">
        <img src="https://pepy.tech/badge/tokenizers/week" />
    </a>
</p>

## ğŸ“– é¡¹ç›®æ¦‚è¿°

HuggingFace Tokenizers æ˜¯å½“ä»Šæœ€æµè¡Œå’Œé«˜æ€§èƒ½çš„åˆ†è¯å™¨å®ç°åº“ï¼Œä¸“æ³¨äºæ€§èƒ½ä¼˜åŒ–å’ŒåŠŸèƒ½å¤šæ ·æ€§ã€‚è¯¥é¡¹ç›®æ˜¯ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„åŸºçŸ³å·¥å…·ï¼Œä¸ºBERTã€GPTã€T5ç­‰ä¸»æµé¢„è®­ç»ƒæ¨¡å‹æä¾›äº†å¼ºå¤§çš„æ–‡æœ¬é¢„å¤„ç†èƒ½åŠ›ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- **ğŸš€ æè‡´æ€§èƒ½**ï¼šåŸºäºRustç¼–å†™çš„æ ¸å¿ƒå¼•æ“ï¼Œåœ¨æœåŠ¡å™¨CPUä¸Šå¤„ç†1GBæ–‡æœ¬ä»…éœ€ä¸åˆ°20ç§’
- **ğŸ”§ æ˜“äºä½¿ç”¨**ï¼šç®€æ´çš„APIè®¾è®¡ï¼ŒåŒæ—¶æä¾›æé«˜çš„çµæ´»æ€§å’Œå¯å®šåˆ¶æ€§
- **ğŸ­ ç”Ÿäº§å°±ç»ª**ï¼šä¸“ä¸ºç ”ç©¶å’Œç”Ÿäº§ç¯å¢ƒè®¾è®¡ï¼Œç¨³å®šå¯é 
- **ğŸ“ å¯¹é½è¿½è¸ª**ï¼šæ ‡å‡†åŒ–å¤„ç†è¿‡ç¨‹ä¸­å§‹ç»ˆä¿æŒåŸå§‹æ–‡æœ¬ä½ç½®æ˜ å°„
- **âš™ï¸ å…¨æµç¨‹å¤„ç†**ï¼šåŒ…å«æˆªæ–­ã€å¡«å……ã€ç‰¹æ®Šæ ‡è®°æ·»åŠ ç­‰å®Œæ•´çš„é¢„å¤„ç†åŠŸèƒ½

## ğŸ—ï¸ æ¶æ„åŸç†æ·±åº¦å‰–æ

### åˆ†è¯å™¨çš„å·¥ä½œåŸç†

Tokenizers é‡‡ç”¨ç®¡é“åŒ–è®¾è®¡ï¼Œå°†æ–‡æœ¬å¤„ç†åˆ†è§£ä¸ºå››ä¸ªæ ¸å¿ƒé˜¶æ®µï¼š

```mermaid
graph TD
    A[åŸå§‹æ–‡æœ¬] --> B[Normalizer æ–‡æœ¬æ ‡å‡†åŒ–]
    B --> C[PreTokenizer é¢„åˆ†è¯]
    C --> D[Model æ ¸å¿ƒåˆ†è¯ç®—æ³•]
    D --> E[PostProcessor åå¤„ç†]
    E --> F[Encoding è¾“å‡ºç»“æœ]
```

#### 1. Normalizerï¼ˆæ ‡å‡†åŒ–å™¨ï¼‰
è´Ÿè´£æ–‡æœ¬æ ‡å‡†åŒ–å¤„ç†ï¼Œç¡®ä¿è¾“å…¥æ–‡æœ¬çš„ä¸€è‡´æ€§ã€‚

**æ·±åº¦è§£æ**ï¼š
- Unicodeæ ‡å‡†åŒ–ï¼šå¤„ç†ä¸åŒç¼–ç å½¢å¼çš„ç›¸åŒå­—ç¬¦
- å¤§å°å†™è½¬æ¢ï¼šç»Ÿä¸€æ–‡æœ¬å¤§å°å†™æ ¼å¼
- å­—ç¬¦æ¸…ç†ï¼šç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦

**ç¤ºä¾‹**ï¼š
```python
from tokenizers.normalizers import NFD, StripAccents, Lowercase

# ç»„åˆå¤šä¸ªæ ‡å‡†åŒ–å™¨
normalizer = Sequence([
    NFD(),          # Unicodeæ ‡å‡†åŒ–
    StripAccents(), # ç§»é™¤é‡éŸ³ç¬¦å·
    Lowercase()     # è½¬æ¢ä¸ºå°å†™
])
```

#### 2. PreTokenizerï¼ˆé¢„åˆ†è¯å™¨ï¼‰
åˆæ­¥åˆ†å‰²æ–‡æœ¬ï¼Œä¸ºåç»­æ¨¡å‹å¤„ç†åšå‡†å¤‡ã€‚

**æ·±åº¦è§£æ**ï¼š
- ç©ºæ ¼åˆ†å‰²ï¼šæœ€åŸºç¡€çš„åˆ†è¯æ–¹å¼
- å­—èŠ‚çº§å¤„ç†ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—
- æ­£åˆ™è¡¨è¾¾å¼ï¼šåŸºäºè§„åˆ™çš„å¤æ‚åˆ†å‰²

**ç¤ºä¾‹**ï¼š
```python
from tokenizers.pre_tokenizers import Whitespace, ByteLevel

# ç®€å•ç©ºæ ¼åˆ†å‰²
pre_tokenizer = Whitespace()

# å­—èŠ‚çº§é¢„å¤„ç†ï¼ˆç”¨äºGPTæ¨¡å‹ï¼‰
byte_level = ByteLevel(add_prefix_space=True)
```

#### 3. Modelï¼ˆæ ¸å¿ƒåˆ†è¯æ¨¡å‹ï¼‰
æ‰§è¡Œå®é™…çš„åˆ†è¯ç®—æ³•ï¼Œè¿™æ˜¯tokenizerçš„æ ¸å¿ƒç»„ä»¶ã€‚

**æ”¯æŒçš„ç®—æ³•**ï¼š

##### BPE (Byte Pair Encoding)
**ç®—æ³•åŸç†**ï¼šé€šè¿‡ç»Ÿè®¡å­—ç¬¦å¯¹é¢‘ç‡ï¼Œé€æ­¥åˆå¹¶é«˜é¢‘å­—ç¬¦å¯¹å½¢æˆæ–°çš„å­è¯å•å…ƒã€‚

**æºç æ·±åº¦è§£æ**ï¼š
```rust
// BPEæ¨¡å‹æ ¸å¿ƒç»“æ„
pub struct BPE {
    vocab: Vocab,                    // è¯æ±‡è¡¨
    vocab_r: VocabR,                // åå‘è¯æ±‡è¡¨
    merges: MergeMap,               // åˆå¹¶è§„åˆ™
    cache: Option<Cache<String, Word>>, // ç¼“å­˜ä¼˜åŒ–
    dropout: Option<f32>,           // Dropoutæ¦‚ç‡
    unk_token: Option<String>,      // æœªçŸ¥è¯æ ‡è®°
}
```

**åº”ç”¨åœºæ™¯**ï¼šGPTç³»åˆ—æ¨¡å‹ã€RoBERTaç­‰

##### WordPiece
**ç®—æ³•åŸç†**ï¼šåŸºäºæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œé€‰æ‹©èƒ½æœ€å¤§åŒ–è®­ç»ƒè¯­æ–™æ¦‚ç‡çš„åˆ†è¯æ–¹æ¡ˆã€‚

**æºç æ·±åº¦è§£æ**ï¼š
```rust
pub struct WordPiece {
    vocab: Vocab,                    // è¯æ±‡è¡¨
    vocab_r: VocabR,                // åå‘è¯æ±‡è¡¨
    unk_token: String,              // æœªçŸ¥è¯æ ‡è®°
    continuing_subword_prefix: String, // å­è¯å‰ç¼€ï¼ˆå¦‚##ï¼‰
    max_input_chars_per_word: usize,   // å•è¯æœ€å¤§å­—ç¬¦æ•°
}
```

**åº”ç”¨åœºæ™¯**ï¼šBERTã€DistilBERTç­‰Googleç³»åˆ—æ¨¡å‹

##### Unigram
**ç®—æ³•åŸç†**ï¼šåŸºäºæ¦‚ç‡çš„å­è¯åˆ†å‰²ï¼Œé€šè¿‡EMç®—æ³•ä¼˜åŒ–å­è¯æ¦‚ç‡åˆ†å¸ƒã€‚

**ç‰¹ç‚¹**ï¼š
- æ”¯æŒå¤šç§åˆ†è¯è·¯å¾„
- åŸºäºæ¦‚ç‡é€‰æ‹©æœ€ä¼˜åˆ†è¯
- é€‚åˆå¤„ç†å„ç§è¯­è¨€

**åº”ç”¨åœºæ™¯**ï¼šT5ã€mT5ã€XLNetç­‰æ¨¡å‹

#### 4. PostProcessorï¼ˆåå¤„ç†å™¨ï¼‰
ä¸ºæ¨¡å‹æ·»åŠ å¿…è¦çš„ç‰¹æ®Šæ ‡è®°å’Œæ ¼å¼åŒ–ã€‚

**åŠŸèƒ½**ï¼š
- æ·»åŠ [CLS]ã€[SEP]ç­‰ç‰¹æ®Šæ ‡è®°
- å¤„ç†å¥å­å¯¹è¾“å…¥æ ¼å¼
- ç”Ÿæˆattention maskå’Œtoken type ids

## ğŸ’» å¤šè¯­è¨€ç»‘å®šæ”¯æŒ

### Python ç»‘å®š
**é¡¹ç›®ç»“æ„**ï¼š
```
bindings/python/
â”œâ”€â”€ src/          # Rustæºç 
â”œâ”€â”€ py_src/       # PythonåŒ…è£…ä»£ç 
â”œâ”€â”€ examples/     # ä½¿ç”¨ç¤ºä¾‹
â””â”€â”€ tests/        # æµ‹è¯•ä»£ç 
```

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- å®Œæ•´çš„Python API
- ç±»å‹æç¤ºæ”¯æŒ
- å¼‚æ­¥æ“ä½œæ”¯æŒ

### Node.js ç»‘å®š
**é¡¹ç›®ç»“æ„**ï¼š
```
bindings/node/
â”œâ”€â”€ src/          # Rustæºç 
â”œâ”€â”€ examples/     # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ lib/          # TypeScriptç±»å‹å®šä¹‰
â””â”€â”€ npm/          # å¤šå¹³å°é¢„ç¼–è¯‘åŒ…
```

**æ ¸å¿ƒç‰¹æ€§**ï¼š
- TypeScriptæ”¯æŒ
- å¤šå¹³å°é¢„ç¼–è¯‘äºŒè¿›åˆ¶
- ç°ä»£JavaScriptè¯­æ³•

### Rust æ ¸å¿ƒ
**é¡¹ç›®ç»“æ„**ï¼š
```
tokenizers/
â”œâ”€â”€ src/          # æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ models/   # åˆ†è¯æ¨¡å‹
â”‚   â”œâ”€â”€ normalizers/ # æ ‡å‡†åŒ–å™¨
â”‚   â”œâ”€â”€ pre_tokenizers/ # é¢„åˆ†è¯å™¨
â”‚   â”œâ”€â”€ processors/ # åå¤„ç†å™¨
â”‚   â””â”€â”€ tokenizer/ # ä¸»è¦æ¥å£
â”œâ”€â”€ benches/      # æ€§èƒ½æµ‹è¯•
â””â”€â”€ examples/     # ä½¿ç”¨ç¤ºä¾‹
```

## ğŸš€ å¿«é€Ÿå¼€å§‹æŒ‡å—

### ç¯å¢ƒå‡†å¤‡

#### Pythonç¯å¢ƒ
```bash
# å®‰è£…å‘å¸ƒç‰ˆæœ¬
pip install tokenizers

# æˆ–å®‰è£…å¼€å‘ç‰ˆæœ¬
pip install git+https://github.com/huggingface/tokenizers.git#subdirectory=bindings/python
```

#### Node.jsç¯å¢ƒ
```bash
npm install tokenizers
```

### åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

#### 1. åˆ›å»ºå¹¶è®­ç»ƒBPEåˆ†è¯å™¨

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# åˆå§‹åŒ–BPEæ¨¡å‹
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))

# è®¾ç½®é¢„åˆ†è¯å™¨
tokenizer.pre_tokenizer = Whitespace()

# é…ç½®è®­ç»ƒå™¨
trainer = BpeTrainer(
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"],
    vocab_size=30000,
    min_frequency=2
)

# è®­ç»ƒåˆ†è¯å™¨
files = ["train.txt", "valid.txt"]
tokenizer.train(files, trainer)

# ä¿å­˜æ¨¡å‹
tokenizer.save("my-tokenizer.json")
```

**æ·±åº¦è§£æ**ï¼š
- `vocab_size=30000`ï¼šè®¾ç½®è¯æ±‡è¡¨å¤§å°ï¼Œå½±å“æ¨¡å‹å®¹é‡å’Œæ€§èƒ½
- `min_frequency=2`ï¼šåªæœ‰å‡ºç°é¢‘ç‡â‰¥2çš„å­—ç¬¦å¯¹æ‰ä¼šè¢«åˆå¹¶
- ç‰¹æ®Šæ ‡è®°åœ¨è®­ç»ƒå‰å°±è¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­

#### 2. åŠ è½½é¢„è®­ç»ƒåˆ†è¯å™¨

```python
from tokenizers import Tokenizer

# ä»æ–‡ä»¶åŠ è½½
tokenizer = Tokenizer.from_file("my-tokenizer.json")

# ä»HuggingFace HubåŠ è½½ï¼ˆéœ€è¦httpç‰¹æ€§ï¼‰
tokenizer = Tokenizer.from_pretrained("bert-base-uncased")
```

#### 3. æ–‡æœ¬ç¼–ç ä¸è§£ç 

```python
# å•å¥ç¼–ç 
encoding = tokenizer.encode("Hello, how are you?")
print(f"Tokens: {encoding.tokens}")
print(f"IDs: {encoding.ids}")
print(f"Offsets: {encoding.offsets}")

# æ‰¹é‡ç¼–ç 
encodings = tokenizer.encode_batch([
    "Hello, how are you?",
    "I'm fine, thank you!"
])

# è§£ç 
decoded = tokenizer.decode(encoding.ids)
print(f"Decoded: {decoded}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
Tokens: ['Hello', ',', 'how', 'are', 'you', '?']
IDs: [7592, 16, 1581, 1419, 1336, 37]
Offsets: [(0, 5), (5, 6), (7, 10), (11, 14), (15, 18), (18, 19)]
Decoded: Hello, how are you?
```

### é«˜çº§é…ç½®ç¤ºä¾‹

#### 1. è‡ªå®šä¹‰BERTé£æ ¼åˆ†è¯å™¨

```python
from tokenizers import Tokenizer
from tokenizers.models import WordPiece
from tokenizers.normalizers import BertNormalizer
from tokenizers.pre_tokenizers import BertPreTokenizer
from tokenizers.processors import BertProcessing

# åˆ›å»ºBERTé£æ ¼åˆ†è¯å™¨
tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))

# è®¾ç½®BERTæ ‡å‡†åŒ–å™¨
tokenizer.normalizer = BertNormalizer(
    clean_text=True,
    handle_chinese_chars=True,
    strip_accents=True,
    lowercase=True
)

# è®¾ç½®BERTé¢„åˆ†è¯å™¨
tokenizer.pre_tokenizer = BertPreTokenizer()

# è®¾ç½®BERTåå¤„ç†å™¨
tokenizer.post_processor = BertProcessing(
    sep=("[SEP]", tokenizer.token_to_id("[SEP]")),
    cls=("[CLS]", tokenizer.token_to_id("[CLS]"))
)
```

#### 2. å­—èŠ‚çº§BPEé…ç½®ï¼ˆGPTé£æ ¼ï¼‰

```python
from tokenizers.models import BPE
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.processors import ByteLevel as ByteLevelProcessor
from tokenizers.decoders import ByteLevel as ByteLevelDecoder

# åˆ›å»ºå­—èŠ‚çº§BPEåˆ†è¯å™¨
tokenizer = Tokenizer(BPE())

# å­—èŠ‚çº§é¢„å¤„ç†
tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)

# å­—èŠ‚çº§åå¤„ç†
tokenizer.post_processor = ByteLevelProcessor()

# å­—èŠ‚çº§è§£ç å™¨
tokenizer.decoder = ByteLevelDecoder()
```

## ğŸ”§ å®é™…åº”ç”¨åœºæ™¯

### 1. å¤§è§„æ¨¡è¯­æ–™è®­ç»ƒ

å¯¹äºå¤§è§„æ¨¡è¯­æ–™åº“è®­ç»ƒï¼ŒTokenizersæä¾›äº†é«˜æ•ˆçš„å¹¶è¡Œå¤„ç†èƒ½åŠ›ï¼š

```python
import glob
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer

# å¤„ç†å¤§é‡æ–‡ä»¶
files = glob.glob("data/**/*.txt", recursive=True)

# é…ç½®å¤§è¯æ±‡é‡è®­ç»ƒå™¨
trainer = BpeTrainer(
    vocab_size=50000,
    min_frequency=5,
    special_tokens=["<pad>", "<unk>", "<s>", "</s>"],
    show_progress=True  # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
)

tokenizer = Tokenizer(BPE())
tokenizer.train(files, trainer)
```

**æ€§èƒ½ä¼˜åŒ–æç¤º**ï¼š
- è®¾ç½®`RAYON_RS_NUM_THREADS`ç¯å¢ƒå˜é‡æ§åˆ¶å¹¶è¡Œçº¿ç¨‹æ•°
- ä½¿ç”¨SSDå­˜å‚¨è®­ç»ƒè¯­æ–™ä»¥æé«˜I/Oæ€§èƒ½
- åˆç†è®¾ç½®`min_frequency`ä»¥å¹³è¡¡è¯æ±‡è¡¨å¤§å°å’Œè¦†ç›–ç‡

### 2. å¤šè¯­è¨€å¤„ç†

```python
# å¤„ç†ä¸­æ–‡æ–‡æœ¬
chinese_tokenizer = Tokenizer(BPE())
chinese_tokenizer.normalizer = BertNormalizer(
    handle_chinese_chars=True,
    strip_accents=False  # ä¿ç•™ä¸­æ–‡å­—ç¬¦
)

# å¤„ç†å¤šç§è¯­è¨€æ··åˆæ–‡æœ¬
multilingual_trainer = BpeTrainer(
    vocab_size=100000,  # å¢å¤§è¯æ±‡è¡¨ä»¥è¦†ç›–å¤šç§è¯­è¨€
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
)
```

### 3. è‡ªå®šä¹‰åˆ†è¯è§„åˆ™

```python
from tokenizers.pre_tokenizers import Split
import re

# è‡ªå®šä¹‰æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯
custom_pre_tokenizer = Split(
    pattern=re.compile(r'\w+|[^\w\s]'),
    behavior="isolated"
)

tokenizer.pre_tokenizer = custom_pre_tokenizer
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### æ€§èƒ½å¯¹æ¯”

åœ¨ç›¸åŒç¡¬ä»¶ç¯å¢ƒä¸‹çš„æ€§èƒ½æµ‹è¯•ç»“æœï¼š

| åˆ†è¯å™¨ | å¤„ç†é€Ÿåº¦ (tokens/sec) | å†…å­˜ä½¿ç”¨ (MB) | æ–‡ä»¶å¤§å° (MB) |
|-------|---------------------|--------------|--------------|
| Tokenizers (Rust) | 1,000,000+ | 50-100 | 5-20 |
| SentencePiece | 200,000 | 100-200 | 10-30 |
| spaCy | 100,000 | 200-500 | 50-100 |

**æ€§èƒ½ä¼˜åŠ¿æ¥æº**ï¼š
1. **Rustæ ¸å¿ƒ**ï¼šé›¶æˆæœ¬æŠ½è±¡ï¼Œå†…å­˜å®‰å…¨ï¼Œæ— GCå¼€é”€
2. **å¹¶è¡Œå¤„ç†**ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
3. **ç¼“å­˜ä¼˜åŒ–**ï¼šæ™ºèƒ½ç¼“å­˜æœºåˆ¶å‡å°‘é‡å¤è®¡ç®—
4. **é›¶æ‹·è´**ï¼šæœ€å°åŒ–å†…å­˜åˆ†é…å’Œæ‹·è´

### åŸºå‡†æµ‹è¯•ä»£ç 

```python
import time
from tokenizers import Tokenizer

# åŠ è½½åˆ†è¯å™¨
tokenizer = Tokenizer.from_file("tokenizer.json")

# å‡†å¤‡æµ‹è¯•æ•°æ®
test_text = "Your test text here..." * 1000

# æ€§èƒ½æµ‹è¯•
start_time = time.time()
for _ in range(1000):
    encoding = tokenizer.encode(test_text)
end_time = time.time()

print(f"å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
print(f"å¤„ç†é€Ÿåº¦: {len(encoding.tokens) * 1000 / (end_time - start_time):.0f} tokens/sec")
```

## ğŸ› ï¸ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 1. å†…å­˜ä½¿ç”¨ä¼˜åŒ–

**é—®é¢˜**ï¼šå¤„ç†å¤§å‹è¯­æ–™æ—¶å†…å­˜å ç”¨è¿‡é«˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ‰¹å¤„ç†å¤§æ–‡ä»¶
def process_large_file(file_path, batch_size=1000):
    with open(file_path, 'r') as f:
        batch = []
        for line in f:
            batch.append(line.strip())
            if len(batch) >= batch_size:
                encodings = tokenizer.encode_batch(batch)
                # å¤„ç†ç¼–ç ç»“æœ
                yield encodings
                batch = []
        if batch:
            yield tokenizer.encode_batch(batch)
```

### 2. ç‰¹æ®Šå­—ç¬¦å¤„ç†

**é—®é¢˜**ï¼šåŒ…å«ç‰¹æ®ŠUnicodeå­—ç¬¦çš„æ–‡æœ¬å¤„ç†ä¸å½“

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
from tokenizers.normalizers import NFD, StripAccents, Replace

# å¤„ç†ç‰¹æ®Šå­—ç¬¦
normalizer = Sequence([
    Replace(pattern="["""]", content='"'),  # ç»Ÿä¸€å¼•å·
    Replace(pattern="[''']", content="'"),  # ç»Ÿä¸€æ’‡å·
    NFD(),
    StripAccents()
])
tokenizer.normalizer = normalizer
```

### 3. è¯æ±‡è¡¨å¤–è¯å¤„ç†

**é—®é¢˜**ï¼šæ¨¡å‹é‡åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„è¯æ±‡

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# è®¾ç½®å›é€€ç­–ç•¥
from tokenizers.models import BPE

bpe = BPE(
    unk_token="[UNK]",
    fuse_unk=True,      # åˆå¹¶è¿ç»­çš„UNKæ ‡è®°
    byte_fallback=True  # ä½¿ç”¨å­—èŠ‚çº§å›é€€
)
```

## ğŸ” æºç æ¶æ„æ·±åº¦å‰–æ

### æ ¸å¿ƒæ¨¡å—è§£æ

#### 1. tokenizeræ¨¡å— (`src/tokenizer/mod.rs`)

**æ ¸å¿ƒæ¥å£å®šä¹‰**ï¼š
```rust
pub trait Model {
    type Trainer: Trainer + Sync;
    
    // æ ¸å¿ƒåˆ†è¯æ–¹æ³•
    fn tokenize(&self, sequence: &str) -> Result<Vec<Token>>;
    
    // è¯æ±‡è¡¨æ“ä½œ
    fn token_to_id(&self, token: &str) -> Option<u32>;
    fn id_to_token(&self, id: u32) -> Option<String>;
    fn get_vocab(&self) -> HashMap<String, u32>;
}
```

**è®¾è®¡æ¨¡å¼**ï¼š
- ç­–ç•¥æ¨¡å¼ï¼šä¸åŒçš„Modelå®ç°ä¸åŒçš„åˆ†è¯ç­–ç•¥
- å»ºé€ è€…æ¨¡å¼ï¼šTokenizerBuilderæä¾›çµæ´»çš„æ„å»ºæ–¹å¼
- æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼šå®šä¹‰æ ‡å‡†çš„åˆ†è¯æµç¨‹

#### 2. modelsæ¨¡å— (`src/models/`)

**BPEå®ç°æ ¸å¿ƒ**ï¼š
```rust
impl Model for BPE {
    fn tokenize(&self, sequence: &str) -> Result<Vec<Token>> {
        // 1. å°†åºåˆ—è½¬æ¢ä¸ºWordå¯¹è±¡
        let word = self.merge_word(sequence)?;
        
        // 2. åº”ç”¨åˆå¹¶è§„åˆ™
        word.merge_all(&self.merges, self.dropout);
        
        // 3. è½¬æ¢ä¸ºTokenåºåˆ—
        Ok(self.word_to_tokens(&word).collect())
    }
}
```

**å…³é”®ç®—æ³•**ï¼š
- åˆå¹¶ç®—æ³•ï¼šåŸºäºä¼˜å…ˆçº§é˜Ÿåˆ—çš„é«˜æ•ˆåˆå¹¶
- ç¼“å­˜æœºåˆ¶ï¼šLRUç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
- Dropoutï¼šè®­ç»ƒæ—¶çš„æ­£åˆ™åŒ–æŠ€æœ¯

### å†…å­˜ç®¡ç†ç­–ç•¥

```rust
// ä½¿ç”¨ç´§å‡‘å­—ç¬¦ä¸²å‡å°‘å†…å­˜å ç”¨
use compact_str::CompactString;

// ä½¿ç”¨é«˜æ•ˆçš„HashMapå®ç°
use ahash::AHashMap;

// æ™ºèƒ½ç¼“å­˜ç®¡ç†
pub struct Cache<K, V> {
    map: AHashMap<K, V>,
    capacity: usize,
}
```

## ğŸš€ é«˜çº§ç‰¹æ€§ä¸æ‰©å±•

### 1. è‡ªå®šä¹‰åˆ†è¯æ¨¡å‹

å®ç°è‡ªå®šä¹‰åˆ†è¯æ¨¡å‹çš„å®Œæ•´ç¤ºä¾‹ï¼š

```python
from tokenizers import Tokenizer
from tokenizers.models import Model

class CustomModel(Model):
    def __init__(self, vocab_file):
        # åŠ è½½è‡ªå®šä¹‰è¯æ±‡è¡¨
        self.vocab = self.load_vocab(vocab_file)
        
    def tokenize(self, text):
        # å®ç°è‡ªå®šä¹‰åˆ†è¯é€»è¾‘
        return self.custom_tokenize_logic(text)
        
    def load_vocab(self, vocab_file):
        # è¯æ±‡è¡¨åŠ è½½é€»è¾‘
        pass
```

### 2. åŠ¨æ€è¯æ±‡è¡¨æ›´æ–°

```python
# åœ¨çº¿å­¦ä¹ æ–°è¯æ±‡
def update_vocabulary(tokenizer, new_texts):
    # ç»Ÿè®¡æ–°è¯é¢‘ç‡
    word_counts = count_words(new_texts)
    
    # é€‰æ‹©é«˜é¢‘æ–°è¯
    new_words = select_high_frequency_words(word_counts)
    
    # æ›´æ–°è¯æ±‡è¡¨
    for word in new_words:
        tokenizer.add_tokens([word])
```

### 3. å¤šæ¨¡æ€æ‰©å±•

```python
# æ”¯æŒå›¾åƒæ ‡è®°çš„åˆ†è¯å™¨
class MultiModalTokenizer:
    def __init__(self, text_tokenizer, image_vocab_size=1000):
        self.text_tokenizer = text_tokenizer
        self.image_vocab_size = image_vocab_size
        
    def encode_text_image(self, text, image_features):
        # ç¼–ç æ–‡æœ¬
        text_tokens = self.text_tokenizer.encode(text)
        
        # é‡åŒ–å›¾åƒç‰¹å¾ä¸ºç¦»æ•£æ ‡è®°
        image_tokens = self.quantize_image_features(image_features)
        
        return text_tokens, image_tokens
```

## ğŸ“š æœ€ä½³å®è·µæŒ‡å—

### 1. è®­ç»ƒæ•°æ®å‡†å¤‡

**æ•°æ®è´¨é‡è¦æ±‚**ï¼š
- æ–‡æœ¬æ¸…æ´ï¼šç§»é™¤HTMLæ ‡ç­¾ã€æ§åˆ¶å­—ç¬¦
- ç¼–ç ç»Ÿä¸€ï¼šç¡®ä¿UTF-8ç¼–ç 
- å»é‡å¤„ç†ï¼šç§»é™¤é‡å¤æ–‡æœ¬
- é•¿åº¦è¿‡æ»¤ï¼šè¿‡æ»¤è¿‡é•¿æˆ–è¿‡çŸ­çš„æ–‡æœ¬

```python
def clean_training_data(texts):
    cleaned = []
    for text in texts:
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        # è¿‡æ»¤é•¿åº¦
        if 10 <= len(text) <= 1000:
            cleaned.append(text)
    return cleaned
```

### 2. æ¨¡å‹é€‰æ‹©ç­–ç•¥

**BPEé€‚ç”¨åœºæ™¯**ï¼š
- å¼€æ”¾åŸŸæ–‡æœ¬
- å¤šè¯­è¨€æ”¯æŒ
- ç”Ÿæˆå¼æ¨¡å‹

**WordPieceé€‚ç”¨åœºæ™¯**ï¼š
- åˆ†ç±»ä»»åŠ¡
- BERTç±»æ¨¡å‹
- éœ€è¦å­è¯å‰ç¼€æ ‡è¯†

**Unigramé€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦æ¦‚ç‡ä¿¡æ¯
- å¤šè·¯å¾„åˆ†è¯
- åºåˆ—åˆ°åºåˆ—ä»»åŠ¡

### 3. æ€§èƒ½è°ƒä¼˜æŠ€å·§

```python
# 1. åˆç†è®¾ç½®ç¼“å­˜å¤§å°
tokenizer.enable_cache(cache_capacity=10000)

# 2. æ‰¹å¤„ç†ä¼˜åŒ–
def batch_encode_efficiently(texts, batch_size=32):
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        encodings = tokenizer.encode_batch(batch)
        results.extend(encodings)
    return results

# 3. å¹¶è¡Œå¤„ç†è®¾ç½®
import os
os.environ['RAYON_RS_NUM_THREADS'] = '8'
```

## ğŸ”§ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### DockeråŒ–éƒ¨ç½²

```dockerfile
FROM python:3.9-slim

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Rustï¼ˆå¦‚æœéœ€è¦ä»æºç ç¼–è¯‘ï¼‰
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"

# å®‰è£…tokenizers
RUN pip install tokenizers

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . /app
WORKDIR /app

CMD ["python", "app.py"]
```

### å¾®æœåŠ¡æ¶æ„

```python
from flask import Flask, request, jsonify
from tokenizers import Tokenizer

app = Flask(__name__)

# å…¨å±€åŠ è½½åˆ†è¯å™¨
tokenizer = Tokenizer.from_file("production-tokenizer.json")

@app.route('/tokenize', methods=['POST'])
def tokenize_text():
    try:
        data = request.get_json()
        text = data['text']
        
        # æ‰§è¡Œåˆ†è¯
        encoding = tokenizer.encode(text)
        
        return jsonify({
            'tokens': encoding.tokens,
            'ids': encoding.ids,
            'attention_mask': encoding.attention_mask
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### è´Ÿè½½å‡è¡¡ä¸ç¼“å­˜

```python
import redis
import pickle
from functools import wraps

# Redisç¼“å­˜è£…é¥°å™¨
def cached_tokenize(expire_time=3600):
    redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    def decorator(func):
        @wraps(func)
        def wrapper(text):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"tokenize:{hash(text)}"
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = redis_client.get(cache_key)
            if cached_result:
                return pickle.loads(cached_result)
            
            # æ‰§è¡Œåˆ†è¯
            result = func(text)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            redis_client.setex(
                cache_key, 
                expire_time, 
                pickle.dumps(result)
            )
            
            return result
        return wrapper
    return decorator

@cached_tokenize(expire_time=1800)
def tokenize_with_cache(text):
    return tokenizer.encode(text)
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### å•å…ƒæµ‹è¯•

```python
import unittest
from tokenizers import Tokenizer

class TestTokenizer(unittest.TestCase):
    def setUp(self):
        self.tokenizer = Tokenizer.from_file("test-tokenizer.json")
    
    def test_basic_tokenization(self):
        text = "Hello, world!"
        encoding = self.tokenizer.encode(text)
        
        # éªŒè¯åŸºæœ¬åŠŸèƒ½
        self.assertIsInstance(encoding.tokens, list)
        self.assertIsInstance(encoding.ids, list)
        self.assertEqual(len(encoding.tokens), len(encoding.ids))
    
    def test_special_tokens(self):
        encoding = self.tokenizer.encode("[CLS] Hello [SEP]")
        
        # éªŒè¯ç‰¹æ®Šæ ‡è®°å¤„ç†
        self.assertIn('[CLS]', encoding.tokens)
        self.assertIn('[SEP]', encoding.tokens)
    
    def test_batch_encoding(self):
        texts = ["Hello, world!", "How are you?"]
        encodings = self.tokenizer.encode_batch(texts)
        
        # éªŒè¯æ‰¹å¤„ç†åŠŸèƒ½
        self.assertEqual(len(encodings), 2)
        for encoding in encodings:
            self.assertIsInstance(encoding.tokens, list)

if __name__ == '__main__':
    unittest.main()
```

### æ€§èƒ½æµ‹è¯•

```python
import time
import psutil
import numpy as np

def benchmark_tokenizer(tokenizer, test_texts, num_runs=100):
    # é¢„çƒ­
    for text in test_texts[:10]:
        tokenizer.encode(text)
    
    # æ€§èƒ½æµ‹è¯•
    times = []
    memory_usage = []
    
    for i in range(num_runs):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss
        
        # æ‰§è¡Œåˆ†è¯
        for text in test_texts:
            encoding = tokenizer.encode(text)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        
        times.append(end_time - start_time)
        memory_usage.append(end_memory - start_memory)
    
    # ç»Ÿè®¡ç»“æœ
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {np.mean(times):.4f}ç§’")
    print(f"å¤„ç†æ—¶é—´æ ‡å‡†å·®: {np.std(times):.4f}ç§’")
    print(f"å¹³å‡å†…å­˜å¢é•¿: {np.mean(memory_usage) / 1024 / 1024:.2f}MB")
    print(f"å³°å€¼å†…å­˜å¢é•¿: {np.max(memory_usage) / 1024 / 1024:.2f}MB")

# è¿è¡ŒåŸºå‡†æµ‹è¯•
test_texts = ["Your test texts here..."]
benchmark_tokenizer(tokenizer, test_texts)
```

## ğŸ”® æœªæ¥å‘å±•æ–¹å‘

### 1. æŠ€æœ¯è¶‹åŠ¿

- **ç¥ç»ç½‘ç»œåˆ†è¯**ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯åˆ†è¯
- **å¤šæ¨¡æ€æ”¯æŒ**ï¼šæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç»Ÿä¸€åˆ†è¯
- **å¢é‡å­¦ä¹ **ï¼šåœ¨çº¿æ›´æ–°è¯æ±‡è¡¨å’Œæ¨¡å‹
- **å‹ç¼©ä¼˜åŒ–**ï¼šæ›´å°çš„æ¨¡å‹æ–‡ä»¶å’Œå†…å­˜å ç”¨

### 2. ç”Ÿæ€ç³»ç»Ÿæ‰©å±•

- **æ›´å¤šè¯­è¨€ç»‘å®š**ï¼šGoã€Javaã€C++ç­‰
- **äº‘åŸç”Ÿæ”¯æŒ**ï¼šKubernetesã€æœåŠ¡ç½‘æ ¼é›†æˆ
- **è¾¹ç¼˜è®¡ç®—**ï¼šç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼ç³»ç»Ÿä¼˜åŒ–
- **å·¥å…·é“¾å®Œå–„**ï¼šå¯è§†åŒ–ã€è°ƒè¯•ã€ç›‘æ§å·¥å…·

### 3. ç®—æ³•åˆ›æ–°

```python
# æœªæ¥å¯èƒ½çš„è‡ªé€‚åº”åˆ†è¯å™¨
class AdaptiveTokenizer:
    def __init__(self, base_tokenizer):
        self.base_tokenizer = base_tokenizer
        self.domain_adapters = {}
    
    def adapt_to_domain(self, domain_texts, domain_name):
        # åŸºäºé¢†åŸŸæ–‡æœ¬è°ƒæ•´åˆ†è¯ç­–ç•¥
        adapter = self.train_domain_adapter(domain_texts)
        self.domain_adapters[domain_name] = adapter
    
    def tokenize(self, text, domain=None):
        if domain and domain in self.domain_adapters:
            return self.domain_adapters[domain].tokenize(text)
        return self.base_tokenizer.encode(text)
```

## ğŸ“– å­¦ä¹ èµ„æºæ¨è

### å®˜æ–¹æ–‡æ¡£
- [HuggingFace Tokenizers æ–‡æ¡£](https://huggingface.co/docs/tokenizers/)
- [API å‚è€ƒ](https://huggingface.co/docs/tokenizers/api/tokenizer)
- [GitHub ä»“åº“](https://github.com/huggingface/tokenizers)

### å­¦æœ¯è®ºæ–‡
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) - BPEåŸå§‹è®ºæ–‡
- [Google's Neural Machine Translation System](https://arxiv.org/abs/1609.08144) - WordPieceç®—æ³•
- [Subword Regularization](https://arxiv.org/abs/1804.10959) - Unigramè¯­è¨€æ¨¡å‹

### å®è·µæ•™ç¨‹
- [HuggingFace Course](https://huggingface.co/course/)
- [Tokenizers å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/tokenizers/quicktour)
- [è‡ªå®šä¹‰åˆ†è¯å™¨æ•™ç¨‹](https://huggingface.co/docs/tokenizers/training_from_memory)

### ç¤¾åŒºèµ„æº
- [HuggingFace è®ºå›](https://discuss.huggingface.co/)
- [Stack Overflow æ ‡ç­¾](https://stackoverflow.com/questions/tagged/huggingface-tokenizers)
- [Reddit NLP ç¤¾åŒº](https://www.reddit.com/r/MachineLearning/)

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒæ­å»º

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/huggingface/tokenizers.git
cd tokenizers

# å®‰è£…Rustå¼€å‘ç¯å¢ƒ
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source ~/.cargo/env

# Pythonç»‘å®šå¼€å‘
cd bindings/python
pip install -e .

# è¿è¡Œæµ‹è¯•
pytest tests/
```

### ä»£ç è´¡çŒ®æµç¨‹

1. **Forkä»“åº“**å¹¶åˆ›å»ºç‰¹æ€§åˆ†æ”¯
2. **ç¼–å†™ä»£ç **å¹¶æ·»åŠ æµ‹è¯•
3. **è¿è¡Œæµ‹è¯•**ç¡®ä¿åŠŸèƒ½æ­£å¸¸
4. **æäº¤PR**å¹¶æè¿°ä¿®æ”¹å†…å®¹
5. **ä»£ç å®¡æŸ¥**å’Œåé¦ˆå¤„ç†
6. **åˆå¹¶ä»£ç **åˆ°ä¸»åˆ†æ”¯

### æ–‡æ¡£è´¡çŒ®

- æ”¹è¿›ç°æœ‰æ–‡æ¡£
- æ·»åŠ ä½¿ç”¨ç¤ºä¾‹
- ç¿»è¯‘å¤šè¯­è¨€ç‰ˆæœ¬
- åˆ¶ä½œè§†é¢‘æ•™ç¨‹

## ğŸ¯ æ€»ç»“

HuggingFace Tokenizers ä½œä¸ºç°ä»£NLPå·¥å…·é“¾çš„æ ¸å¿ƒç»„ä»¶ï¼Œå‡­å€Ÿå…¶å“è¶Šçš„æ€§èƒ½ã€ä¸°å¯Œçš„åŠŸèƒ½å’Œè‰¯å¥½çš„ç”Ÿæ€ç³»ç»Ÿæ”¯æŒï¼Œå·²ç»æˆä¸ºäº†è¡Œä¸šæ ‡å‡†ã€‚æ— è®ºæ‚¨æ˜¯NLPç ”ç©¶è€…ã€å·¥ç¨‹å¸ˆè¿˜æ˜¯å­¦ç”Ÿï¼ŒæŒæ¡Tokenizerséƒ½å°†ä¸ºæ‚¨çš„é¡¹ç›®å¸¦æ¥å·¨å¤§ä»·å€¼ã€‚

é€šè¿‡æœ¬æŒ‡å—çš„å­¦ä¹ ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š

âœ… ç†è§£åˆ†è¯å™¨çš„æ ¸å¿ƒåŸç†å’Œæ¶æ„è®¾è®¡  
âœ… ç†Ÿç»ƒä½¿ç”¨å„ç§åˆ†è¯ç®—æ³•ï¼ˆBPEã€WordPieceã€Unigramï¼‰  
âœ… æ„å»ºå’Œè®­ç»ƒè‡ªå®šä¹‰åˆ†è¯å™¨  
âœ… åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²å’Œä¼˜åŒ–åˆ†è¯æœåŠ¡  
âœ… è§£å†³å¸¸è§é—®é¢˜å’Œæ€§èƒ½ç“¶é¢ˆ  
âœ… å‚ä¸å¼€æºç¤¾åŒºè´¡çŒ®  

éšç€AIæŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œåˆ†è¯æŠ€æœ¯ä¹Ÿåœ¨æŒç»­æ¼”è¿›ã€‚ä¿æŒå­¦ä¹ å’Œå®è·µï¼Œå…³æ³¨ç¤¾åŒºåŠ¨æ€ï¼Œå°†å¸®åŠ©æ‚¨åœ¨è¿™ä¸ªå¿«é€Ÿå‘å±•çš„é¢†åŸŸä¸­ä¿æŒç«äº‰ä¼˜åŠ¿ã€‚

---

> **æç¤º**ï¼šæœ¬æ–‡æ¡£å°†æŒç»­æ›´æ–°ï¼Œä»¥åæ˜ æœ€æ–°çš„æŠ€æœ¯å‘å±•å’Œæœ€ä½³å®è·µã€‚å»ºè®®æ”¶è—å¹¶å®šæœŸæŸ¥çœ‹æ›´æ–°å†…å®¹ã€‚

> **è”ç³»æ–¹å¼**ï¼šå¦‚æœ‰ç–‘é—®æˆ–å»ºè®®ï¼Œæ¬¢è¿é€šè¿‡GitHub Issuesæˆ–HuggingFaceç¤¾åŒºè®¨è®ºã€‚

**å¼€å§‹æ‚¨çš„Tokenizersä¹‹æ—…å§ï¼** ğŸš€